{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduccion "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ¿Qué es el SEO?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El posicionamiento en buscadores u optimización de motores de búsqueda es el proceso de mejorar la visibilidad de un sitio web en los resultados orgánicos (los que no son pagados), de los diferentes buscadores. También es frecuente nombrarlo por su título inglés, SEO (Search Engine Optimization).\n",
    "\n",
    "**De la definición debemos quedarnos con dos ideas:**\n",
    "\n",
    "* Aumento de la visibilidad: el objetivo del SEO es hacer que lleguen más visitas a una página.\n",
    "* Posicionamiento en buscadores: aunque Google es el rey de los motores de búsqueda (y siempre que hablamos de SEO nos referimos a Google), el significado de SEO se aplica a todos los buscadores (Bing, Yahoo, etc).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/ejemplo-busqueda-seo2.png\" width=\"800\" height=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ¿Por qué es el SEO importante?\n",
    "La razón más importante por la que es necesario el SEO es porque hace más útil tu página web tanto para los usuarios como para los motores de búsqueda. Aunque estos aún no pueden ver una página web como lo hace un humano. El SEO es necesario para ayudar a los motores de búsqueda a entender sobre qué trata cada página y si es o no útil para los usuarios.\n",
    "\n",
    "Ademas presenta  varias ventajas al implementar una estrategia SEO\n",
    "* Aumenta la visibilidad de una marca\n",
    "* Atrae tráfico cualificado\n",
    "* Genera oportunidades de ventas\n",
    "* Educar el mercado\n",
    "* Mejor rendimiento sobre la inversión"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keyword Research\n",
    "**Las keywords son términos utilizados en los buscadores para expresar la información que los usuarios quieren encontrar en Internet.**\n",
    "\n",
    "Lo que separa el contenido de los usuarios se llama palabras clave o Keywords. Una palabra clave es el término que digitan los usuarios en los buscadores para encontrar contenidos que solucionen sus inquietudes.\n",
    "\n",
    "Por ejemplo, si una persona está planeando un viaje a Japón seguramente digitará “mejores destinos en Japón” o “los destinos más económicos de Japón” en la barra de búsqueda. Debemos realizar una investigación de palabras clave\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El análisis y selección de keywords va a determinar casi todos los pasos que a seguir en la web.\n",
    "\n",
    "Desde la elección del dominio (aunque no siempre), pasando por los contenidos y continuando con la estrategia de enlaces.\n",
    "\n",
    "Por eso merece la pena realizar este esfuerzo al principio, porque no es de gusto descubrir que las palabras clave que se han escogido no aportan visitas, o atraen a los visitantes equivocados.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ¿Qué Keyword debería elegir?\n",
    "Deberían ser esas keywords que mejor definen el servicio web , los objetivos o el modelo de negocio.\n",
    "\n",
    " * #### Comenzar con las Keywords más genéricas.\n",
    " \n",
    "* #### Depurar la lista de Keywords enfocado a el servicio web.\n",
    "\n",
    "* #### Realizar una busqueda con palabras más especificas.\n",
    "\n",
    "* #### Depurar la nueva lista enfocado a la web."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Codigo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definimos las funciones Iniciales\n",
    "Vamos a definir las funciones iniciales para el Keyword research de nuestro proyecto."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funcion para definir el idioma y la palabra clave a utilizar.\n",
    "Aqui vamos a elegir nuestra Keyword y además, el idioma y el país donde vamos a realizar la busqueda."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_values():\n",
    "    print(\"Language:\")\n",
    "    print(\"(1) Español\")\n",
    "    print(\"(2) English\")\n",
    "    language = input()\n",
    "    if int(language) == 1:\n",
    "        print (\"Cuál es su Keyword?\")\n",
    "        Keyword= input()\n",
    "        print(\" \\n Tu palabra es:\", Keyword, \"\\n\")\n",
    "        LANG = \"es\"\n",
    "        CONT=\"CO\"\n",
    "        return Keyword, LANG, CONT\n",
    "    if int(language) == 2:\n",
    "        print(\"What is your keyword?\")\n",
    "        Keyword= input()\n",
    "        print(\"Your word is:\", Keyword)\n",
    "        LANG = \"en\"\n",
    "        CONT=\"US\"\n",
    "        return Keyword, LANG, CONT\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funciones para escoger una nueva palabra y realizar todo el filtro.\n",
    "En el caso que queramos escoger una nueva keyword, por medio de la función ya podremos aplicar todas las funciones "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def news():\n",
    "    if CONT == 'US':\n",
    "        print('Do you want to add another Keyword?')\n",
    "        print('(1) Yes')\n",
    "        print('(2) No')\n",
    "        choice = input()\n",
    "    if CONT == 'CO':\n",
    "        print('¿Desea añadir otra Keyword?')\n",
    "        print('(1) Si')\n",
    "        print('(2) No')\n",
    "        choice = input()\n",
    "    return choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_keyword():\n",
    "    a,b,c = set_values()\n",
    "    erasecsvdata()\n",
    "    downdata(a, b, c)\n",
    "    data = reading_data(a)\n",
    "    data = clean_zeros(data)\n",
    "    keywording = Keymarcas(data.Keyword)\n",
    "    print('{} Keywords'.format(len(keywording)))\n",
    "    getalldata(keywording,a)\n",
    "    dates= rec_data(a)\n",
    "    data = pd.concat([ keywords_data, dates])\n",
    "    Key = keys(data)\n",
    "    Key = textkey(Key)\n",
    "    return Key,data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Función para eliminar datos con Keywords anteriores o innecesarios.\n",
    "En el caso de tener algun csv repetido, o con alguna palabra que no vayamos a utilizar podemos eliminarlo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "path = Path.cwd()\n",
    "import shutil, os, sys\n",
    "\n",
    "def erasecsvdata():\n",
    "    csv_files=list(filter(lambda x: '.csv' in x, os.listdir(path)))\n",
    "    if CONT == 'CO':\n",
    "        print ('Estos son los CSV en tu carpeta')\n",
    "        for i in range(len(csv_files)):\n",
    "            print ('({}) -> {}'.format(i+1, csv_files[i]))\n",
    "        print ('Escoge los numeros de los CSV que quieres eliminar o 0 para pasar (ejemplo 1,3)')\n",
    "        numero = input()\n",
    "    if CONT == 'US':\n",
    "        print('This are the CSV on your folder')\n",
    "        for i in range(len(csv_files)):\n",
    "            print('({} -> {})'.format(i+1, csv_files[i]))\n",
    "        print('Choose the numbers of the CSV that you want to delete or choose 0 to pass (example 1,3)')\n",
    "        numero = input()\n",
    "    A = numero.split(',')\n",
    "    A = [int(integer) for integer in A]\n",
    "    for j in range(len(csv_files)):\n",
    "        #print(csv_files[j])\n",
    "        if j+1 in A:\n",
    "            os.remove(csv_files[j])\n",
    "    print('Los archivos restantes son:')\n",
    "    print (list(filter(lambda x: '.csv' in x, os.listdir(path))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funciones para descarga de datos\n",
    "Luego de definir nuestra Keyword objetivo, creamos las funciones para realizar la busqueda."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Función para descargar los datos de la Keyword Principal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver import Chrome\n",
    "from selenium.webdriver.support.ui import Select\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "\n",
    "import time\n",
    "\n",
    "\n",
    "def downdata(Keyword,LANG, CONT):\n",
    "    if os.path.exists( Keyword +'.csv') == False:\n",
    "        options = webdriver.ChromeOptions()\n",
    "        prefs = {\"download.default_directory\": str(path)}\n",
    "        options.add_experimental_option(\"prefs\", prefs)\n",
    "        browser = webdriver.Chrome(executable_path='chromedriver', options = options) \n",
    "        browser.get('https://cocolyze.com/en/google-keyword-planner-tool#null')\n",
    "    \n",
    "        username = browser.find_element_by_id('keyword')\n",
    "        username.send_keys(Keyword)\n",
    "\n",
    "\n",
    "        select_Country = Select(browser.find_element_by_id('country'))\n",
    "        select_Country.select_by_value(CONT)\n",
    "\n",
    "        select_lang = Select(browser.find_element_by_id(\"lang\"))\n",
    "        select_lang.select_by_value(LANG)\n",
    "\n",
    "        button = browser.find_element_by_id('submitBtn')\n",
    "        button.click()\n",
    "\n",
    "\n",
    "        time.sleep(15)\n",
    "\n",
    "        ids = browser.find_elements_by_xpath(\"//*[@href]\")\n",
    "        for ii in ids:\n",
    "            #print (ii.text)\n",
    "            if ii.text == 'CSV':\n",
    "                download = ii\n",
    "        #print (download.text)\n",
    "        download.click()\n",
    "\n",
    "\n",
    "        time.sleep(5)\n",
    "        browser.close()\n",
    "        os.rename('suggestion.csv', Keyword+'.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Función para descargar los datos para todos los keywords de una lista."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getalldata(Keywording,Keyword):\n",
    "    dire = \"Data_\"+ Keyword\n",
    "    try:\n",
    "    # Create target Directory\n",
    "        os.mkdir(dire)\n",
    "        print(\"Directory Created\") \n",
    "    except FileExistsError:\n",
    "        print(\"Directory already exists\")\n",
    "    options = webdriver.ChromeOptions()\n",
    "    prefs = {\"download.default_directory\": str(path)}\n",
    "    options.add_experimental_option(\"prefs\", prefs)\n",
    "    browser = webdriver.Chrome(executable_path='chromedriver', options = options) \n",
    "    browser.get('https://cocolyze.com/en/google-keyword-planner-tool#null')\n",
    "    for i in range(len(Keywording)):\n",
    "        if os.path.exists('Data_'+ Keyword +'/'+ Keywording[i]+str(i+1) +'.csv'):\n",
    "            continue\n",
    "        elif os.path.exists('Data_'+ Keyword+'/'+ Keywording[i] +'.csv')== False :\n",
    "            time.sleep(1)\n",
    "            username = browser.find_element_by_id('keyword')\n",
    "            username.clear()\n",
    "            username.send_keys(Keywording[i])\n",
    "\n",
    "            select_Country = Select(browser.find_element_by_id('country'))\n",
    "            select_Country.select_by_value(CONT)\n",
    "            select_lang = Select(browser.find_element_by_id(\"lang\"))\n",
    "            select_lang.select_by_value(LANG)\n",
    "            button = browser.find_element_by_id('submitBtn')\n",
    "            button.click()\n",
    "            time.sleep(15)\n",
    "            ids = browser.find_elements_by_xpath(\"//*[@href]\")\n",
    "            for ii in ids:\n",
    "                #print (ii.text)\n",
    "                if ii.text == 'CSV':\n",
    "                    download = ii\n",
    "            #print (download.text)\n",
    "            download.click()\n",
    "\n",
    "            time.sleep(3)\n",
    "  \n",
    "            os.rename('suggestion.csv',  Keywording[i]+str(i+1)+'.csv')\n",
    "    \n",
    "            shutil.move(Keywording[i]+str(i+1)+'.csv', str(path)+\"\\Data_\"+ Keyword)\n",
    "            time.sleep(2)\n",
    "    print(\"All data downloaded\")\n",
    "    browser.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entramos los valores y Keyword para nuestro SEO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Language:\n",
      "(1) Español\n",
      "(2) English\n",
      "2\n",
      "What is your keyword?\n",
      "Pants\n",
      "Your word is: Pants\n"
     ]
    }
   ],
   "source": [
    "Keyword, LANG, CONT = set_values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Borramos archivos innecesarios de alguna keyword erronea."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This are the CSV on your folder\n",
      "(1 -> all_data_copy.csv)\n",
      "(2 -> all_data_Pants.csv)\n",
      "(3 -> all_data_sweatshirt.csv)\n",
      "(4 -> nikeid sweatshirt1.csv)\n",
      "(5 -> Pants.csv)\n",
      "(6 -> sweatshirt.csv)\n",
      "Choose the numbers of the CSV that you want to delete or choose 0 to pass (example 1,3)\n",
      "1,2,3,4\n",
      "Los archivos restantes son:\n",
      "['Pants.csv', 'sweatshirt.csv']\n"
     ]
    }
   ],
   "source": [
    "erasecsvdata()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Descargamos los datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "downdata(Keyword,LANG, CONT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lectura y limpieza de datos\n",
    "Ahora vamos a leer y limpiar el dataset descargado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Leemos el dataset bajo las columnas que necesitamos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   Keyword  Search Volume   CPC\n",
      "0           carolina pants         823000  1.11\n",
      "1  sponge bob square pants         246000  0.94\n",
      "2              sweat pants         165000  1.06\n",
      "3               pants yoga         165000  1.27\n",
      "4               yoga pants         165000  1.83\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "def reading_data(Keyword):\n",
    "    col_list = [\"Keyword\",\"Search Volume\",\"CPC\"]\n",
    "    keyword_Data=pd.read_csv(Keyword+'.csv', sep =\";\",usecols=col_list)\n",
    "    print(keyword_Data.head())\n",
    "    return keyword_Data\n",
    "keyword_Data = reading_data(Keyword)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Revisamos si existen datos vacios y eliminamos las filas con datos iguales a cero.\n",
    "Quitamos todos los datos con algún valor a cero ya que esto significa que no hay informacion acerca del volumen de busqueda o del costo por clic para la palabra correspondiente. Además, en caso que eliminemos más del 40% de la informacion, lanzamos una advertencia ya que es una perdida muy grande de información."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Null Data in Keyword = 0\n",
      "\n",
      "\n",
      "Null Data in Search Volume = 0\n",
      "\n",
      "\n",
      "Null Data in CPC = 0\n",
      "\n",
      "\n",
      "Great Dataset! Continue\n"
     ]
    }
   ],
   "source": [
    "def clean_zeros(keyword_Data):\n",
    "    col_names= keyword_Data.columns.tolist()\n",
    "    for column in col_names:\n",
    "        print(\"Null Data in {} = {}\".format(column,keyword_Data[column].isnull().sum()))\n",
    "        print(\"\\n\")\n",
    "    Total = len(keyword_Data)\n",
    "    for column in col_names[1:-1]:\n",
    "         fixed = keyword_Data.loc[keyword_Data[column] > 0]\n",
    "            \n",
    "    Ft= len(fixed)\n",
    "    rang = (Ft/Total)*100\n",
    "   \n",
    "    if CONT == 'CO':\n",
    "        if rang < 60:\n",
    "            print ('Se presenta más del 40% de datos sucios, se recomienda usar otra palabra')\n",
    "        else :\n",
    "            print('Tienes un buen dataset, continua')\n",
    "    if CONT == 'US': \n",
    "        if rang < 60:\n",
    "            print('The Data has 40% of Null data, you should use another Keyword')\n",
    "        else:\n",
    "            print('Great Dataset! Continue')\n",
    "    \n",
    "    return fixed\n",
    "\n",
    "\n",
    "\n",
    "keyword_Data = clean_zeros(keyword_Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Keyword</th>\n",
       "      <th>Search Volume</th>\n",
       "      <th>CPC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>carolina pants</td>\n",
       "      <td>823000</td>\n",
       "      <td>1.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sponge bob square pants</td>\n",
       "      <td>246000</td>\n",
       "      <td>0.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sweat pants</td>\n",
       "      <td>165000</td>\n",
       "      <td>1.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>pants yoga</td>\n",
       "      <td>165000</td>\n",
       "      <td>1.27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>yoga pants</td>\n",
       "      <td>165000</td>\n",
       "      <td>1.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>pants</td>\n",
       "      <td>135000</td>\n",
       "      <td>1.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>nike sweat pants</td>\n",
       "      <td>135000</td>\n",
       "      <td>0.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>men's jeans</td>\n",
       "      <td>135000</td>\n",
       "      <td>2.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>shirt</td>\n",
       "      <td>135000</td>\n",
       "      <td>0.86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>tops</td>\n",
       "      <td>135000</td>\n",
       "      <td>0.97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>pants cargo</td>\n",
       "      <td>110000</td>\n",
       "      <td>0.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>cargo pants</td>\n",
       "      <td>110000</td>\n",
       "      <td>0.97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>khaki pants</td>\n",
       "      <td>90500</td>\n",
       "      <td>1.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>the sisterhood of the traveling pants</td>\n",
       "      <td>90500</td>\n",
       "      <td>0.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>pants dickies</td>\n",
       "      <td>74000</td>\n",
       "      <td>0.87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>dickie pants</td>\n",
       "      <td>74000</td>\n",
       "      <td>0.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>khaki</td>\n",
       "      <td>74000</td>\n",
       "      <td>1.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>women's suit pants</td>\n",
       "      <td>60500</td>\n",
       "      <td>0.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>pants suit womens</td>\n",
       "      <td>60500</td>\n",
       "      <td>0.93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>sweater pants womens</td>\n",
       "      <td>60500</td>\n",
       "      <td>1.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>cargo pants womens</td>\n",
       "      <td>60500</td>\n",
       "      <td>0.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>plaid pants</td>\n",
       "      <td>60500</td>\n",
       "      <td>0.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>women&amp;#39;s cargo pants</td>\n",
       "      <td>60500</td>\n",
       "      <td>0.85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>women's pants suits</td>\n",
       "      <td>60500</td>\n",
       "      <td>0.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>men's sweat pants</td>\n",
       "      <td>60500</td>\n",
       "      <td>2.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>palazzo pants</td>\n",
       "      <td>60500</td>\n",
       "      <td>0.87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>women cargo pants</td>\n",
       "      <td>60500</td>\n",
       "      <td>0.97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>sisterhood of the travelling pants</td>\n",
       "      <td>60500</td>\n",
       "      <td>0.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>suit pants women's</td>\n",
       "      <td>60500</td>\n",
       "      <td>0.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>cargo pants for womens</td>\n",
       "      <td>60500</td>\n",
       "      <td>0.95</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  Keyword  Search Volume   CPC\n",
       "0                          carolina pants         823000  1.11\n",
       "1                 sponge bob square pants         246000  0.94\n",
       "2                             sweat pants         165000  1.06\n",
       "3                              pants yoga         165000  1.27\n",
       "4                              yoga pants         165000  1.83\n",
       "5                                   pants         135000  1.04\n",
       "6                        nike sweat pants         135000  0.49\n",
       "7                             men's jeans         135000  2.12\n",
       "8                                   shirt         135000  0.86\n",
       "9                                    tops         135000  0.97\n",
       "10                            pants cargo         110000  0.99\n",
       "11                            cargo pants         110000  0.97\n",
       "12                            khaki pants          90500  1.54\n",
       "13  the sisterhood of the traveling pants          90500  0.22\n",
       "14                          pants dickies          74000  0.87\n",
       "15                           dickie pants          74000  0.77\n",
       "16                                  khaki          74000  1.26\n",
       "17                     women's suit pants          60500  0.80\n",
       "18                      pants suit womens          60500  0.93\n",
       "19                   sweater pants womens          60500  1.12\n",
       "20                     cargo pants womens          60500  0.92\n",
       "21                            plaid pants          60500  0.74\n",
       "22                women&#39;s cargo pants          60500  0.85\n",
       "23                    women's pants suits          60500  0.96\n",
       "24                      men's sweat pants          60500  2.04\n",
       "25                          palazzo pants          60500  0.87\n",
       "26                      women cargo pants          60500  0.97\n",
       "27     sisterhood of the travelling pants          60500  0.12\n",
       "28                     suit pants women's          60500  0.96\n",
       "29                 cargo pants for womens          60500  0.95"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keyword_Data.head(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hacemos un filtro sobre las marcas que queremos analizar.\n",
    "Queremos filtrar nuestro dataset respecto a las palabras que estan relacionadas con los productos o marcas que queremos \"ofrecer\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "Marcas=['Nike', 'Adidas','Puma','Nautica','Levis', 'Under Armour', 'Zara', 'Diadora', 'Carolina']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **FUZZY WUZZY**\n",
    "FuzzyWuzzy es una biblioteca de Python que se utiliza para la coincidencia de cadenas de texto. La coincidencia de cadenas difusa es el proceso de encontrar cadenas que coinciden con un patrón dado. Básicamente utiliza la distancia de **Levenshtein** para calcular las diferencias entre secuencias.\n",
    "\n",
    "\n",
    "### Distancia de Levenshtein\n",
    "La distancia de Levenshtein es una métrica para medir la diferencia entre dos secuencias. Informalmente, la distancia de Levenshtein entre dos palabras es el número mínimo de ediciones de un solo carácter (es decir, inserciones, eliminaciones o sustituciones) requeridas para cambiar una palabra en la otra."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Pinnzon\\anaconda3\\lib\\site-packages\\fuzzywuzzy\\fuzz.py:11: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning\n",
      "  warnings.warn('Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning')\n"
     ]
    }
   ],
   "source": [
    "from fuzzywuzzy import process, fuzz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_matches(query, choices, limit=30):\n",
    "    results = process.extract(query, choices, limit = limit, scorer = fuzz.partial_ratio)\n",
    "    return results "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hacemos un filtro sobre nuestro conjunto de palabras con aquellas que tengan mas de  un 90% de coincidencia con nuestras marcas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Keymarcas(Keyword):\n",
    "    keywords=[]\n",
    "    for marca in Marcas:\n",
    "        data = get_matches(marca,Keyword)\n",
    "        for i in range(len(data)):\n",
    "            if data[i][1] > 90:\n",
    "                keywords.append(data[i][0]) \n",
    "    for i in range(len(keywords)):\n",
    "        keywords[i]=keywords[i].replace('.','')\n",
    "    return keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords = Keymarcas(keyword_Data.Keyword)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['nike sweat pants',\n",
       " 'nike womens sweat pants',\n",
       " 'nike sweat pants for men',\n",
       " \"nike men's sweat pants\",\n",
       " \"men's nike pants\",\n",
       " 'nike pants mens',\n",
       " 'nike pants for men',\n",
       " 'nike mens pants',\n",
       " 'track pants nike',\n",
       " 'nike track pants',\n",
       " 'nike fleece tech pants',\n",
       " \"nike women's pants\",\n",
       " \"women's nike pants\",\n",
       " \"nike pants women's\",\n",
       " 'nike gold pants',\n",
       " 'nikelab womens pants',\n",
       " 'nike tech fleece pants',\n",
       " 'nike pants tech fleece',\n",
       " 'adidas pants',\n",
       " 'adidas tracksuit pants',\n",
       " 'tracksuit pants adidas',\n",
       " 'track pants adidas',\n",
       " 'adidas pants womens',\n",
       " 'adidas women pants',\n",
       " 'adidas pants for men',\n",
       " \"women's adidas pants\",\n",
       " \"adidas men's pants\",\n",
       " 'men adidas pants',\n",
       " 'adidas tiro 17 pants',\n",
       " 'adidas pants red',\n",
       " 'adidas pants soccer',\n",
       " 'adidas pants tiro 17',\n",
       " 'adidas soccer pants',\n",
       " 'adidas red pants',\n",
       " 'under armor pants',\n",
       " 'underarmour sweat pants',\n",
       " 'carolina pants']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keywords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Para cada palabra encontrada, volvemos a realizar un Keyword Research"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory already exists\n",
      "All data downloaded\n"
     ]
    }
   ],
   "source": [
    "getalldata(keywords ,Keyword)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Leemos y concatenamos todas las palabras encontradas por cada Keyword de nuestra lista filtrada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rec_data(Keyword):\n",
    "    path_2 = 'Data_'+Keyword\n",
    "    files = [file for file in os.listdir(path_2) if not file.startswith('.')] # Ignore hidden files\n",
    "    col_list = [\"Keyword\",\"Search Volume\",\"CPC\"]\n",
    "    keywords_data = pd.DataFrame()\n",
    "\n",
    "    for file in files:\n",
    "        current_data = pd.read_csv(path_2+\"/\"+file, sep =\";\", usecols = col_list)\n",
    "        keywords_data = pd.concat([keywords_data, current_data])\n",
    "    for  i in range(len(keywords)):\n",
    "        keywords_data= pd.concat([keywords_data,keyword_Data.loc[keyword_Data['Keyword']==keywords[2]]])\n",
    "    \n",
    "    keywords_data.to_csv(\"all_data_\"+Keyword+\".csv\", index=False)\n",
    "    return keywords_data\n",
    "keywords_data = rec_data(Keyword)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Keyword</th>\n",
       "      <th>Search Volume</th>\n",
       "      <th>CPC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>adidas pants men's</td>\n",
       "      <td>18100</td>\n",
       "      <td>1.46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>adidas men's pants</td>\n",
       "      <td>18100</td>\n",
       "      <td>1.46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>men's adidas pants</td>\n",
       "      <td>18100</td>\n",
       "      <td>1.46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>adidas joggers men</td>\n",
       "      <td>18100</td>\n",
       "      <td>1.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>adidas joggers women's</td>\n",
       "      <td>18100</td>\n",
       "      <td>1.34</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Keyword  Search Volume   CPC\n",
       "0      adidas pants men's          18100  1.46\n",
       "1      adidas men's pants          18100  1.46\n",
       "2      men's adidas pants          18100  1.46\n",
       "3      adidas joggers men          18100  1.84\n",
       "4  adidas joggers women's          18100  1.34"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keywords_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Eliminamos todas las palabras que tengan un volumen de busqueda menor al promedio y aquellas que esten repetidas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keys(keywords_data):\n",
    "    keywords_data = keywords_data.sort_values(by='Search Volume', ascending=False)\n",
    "    keywords_data= keywords_data.reset_index(drop=True)\n",
    "\n",
    "    keywords_data= keywords_data.loc[keywords_data['Search Volume']> keywords_data['Search Volume'].mean()]\n",
    "\n",
    "    Key= keywords_data['Keyword'].values.tolist()\n",
    "    Key = list(dict.fromkeys(Key))\n",
    "    return Key\n",
    "Key = keys(keywords_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extraemos las palabras como texto y eliminamos cualquier mayuscula para un menor procesamiento de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['nike',\n",
       " 'nfl schedule',\n",
       " 'adidas',\n",
       " 'nfl news',\n",
       " 'under armour',\n",
       " 'nike shoes',\n",
       " 'carolina pants',\n",
       " 'patriots schedule',\n",
       " 'panthers',\n",
       " 'nike outlet',\n",
       " 'adidas shoes',\n",
       " 'nfl shop',\n",
       " 'saints schedule',\n",
       " 'nike shoes for men',\n",
       " 'under armour outlet',\n",
       " 'nike backpacks',\n",
       " 'sweatpants',\n",
       " 'nike air max womens',\n",
       " 'adidas womens shoes',\n",
       " 'nike sweatpants',\n",
       " 'under armour shoes',\n",
       " 'nike sweat pants',\n",
       " 'nike shorts',\n",
       " 'adidas shoes for men',\n",
       " 'adidas outlet',\n",
       " 'joggers for men',\n",
       " 'nike windbreaker',\n",
       " \"men's joggers\",\n",
       " 'nike women',\n",
       " 'nike headbands']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def textkey(Key):\n",
    "    Key = [text.lower() for text in Key]\n",
    "    return Key\n",
    "Key = textkey(Key)\n",
    "Key[:30]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Insertar otra Keyword\n",
    "En el caso que queramos combinar nuestra palabra inicial y queremos añadir más relacionadas a nuestro producto aqui lo podremos realizar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Do you want to add another Keyword?\n",
      "(1) Yes\n",
      "(2) No\n",
      "1\n",
      "Language:\n",
      "(1) Español\n",
      "(2) English\n",
      "2\n",
      "What is your keyword?\n",
      "sweatshirt\n",
      "Your word is: sweatshirt\n",
      "This are the CSV on your folder\n",
      "(1 -> all_data_Pants.csv)\n",
      "(2 -> Pants.csv)\n",
      "(3 -> sweatshirt.csv)\n",
      "Choose the numbers of the CSV that you want to delete or choose 0 to pass (example 1,3)\n",
      "0\n",
      "Los archivos restantes son:\n",
      "['all_data_Pants.csv', 'Pants.csv', 'sweatshirt.csv']\n",
      "               Keyword  Search Volume   CPC\n",
      "0           sweatshirt         165000  1.05\n",
      "1      hoodies for men         165000  1.18\n",
      "2  champion sweatshirt         110000  0.68\n",
      "3  sweatshirt champion         110000  0.66\n",
      "4    nikeid sweatshirt          74000  0.50\n",
      "Null Data in Keyword = 0\n",
      "\n",
      "\n",
      "Null Data in Search Volume = 0\n",
      "\n",
      "\n",
      "Null Data in CPC = 0\n",
      "\n",
      "\n",
      "Great Dataset! Continue\n",
      "62 Keywords\n",
      "Directory already exists\n",
      "All data downloaded\n",
      "Do you want to add another Keyword?\n",
      "(1) Yes\n",
      "(2) No\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    choice = news()\n",
    "    if int(choice) == 1:\n",
    "        Key,data = new_keyword()\n",
    "    else:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ultimo Filtro\n",
    "Realizamos un ultimo filtro con las marcas seleccionadas en caso que hayan keywords poco relacionadas de las nuevas busquedas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "Key = Keymarcas(Key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['nike',\n",
       " 'nike shoes',\n",
       " 'nike outlet',\n",
       " 'nike hoodie',\n",
       " 'nike hoodies',\n",
       " 'hoodies nike',\n",
       " 'nike shoes for men',\n",
       " 'nike backpacks',\n",
       " 'nike sweatpants',\n",
       " 'nike sweat pants',\n",
       " 'nike id',\n",
       " 'nike air max womens',\n",
       " 'nike shorts',\n",
       " 'nike windbreaker',\n",
       " 'nike sweatsuit',\n",
       " 'nike joggers',\n",
       " 'nike sweatshirts',\n",
       " 'nike sweatshirt',\n",
       " 'nike t shirt',\n",
       " 'nike white shoes',\n",
       " 'nike jacket',\n",
       " 'nike women',\n",
       " 'nikeid sweatshirt',\n",
       " 'nike headbands',\n",
       " 'sweatshirts nike',\n",
       " 'nike hoodie men',\n",
       " 'nike leggings',\n",
       " \"men's nike hoodie\",\n",
       " 'nike hoodies men',\n",
       " 'sweatshirt nike']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Key[:30]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generando textos por medio LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Caso 1\n",
    "Vamos a realizar una RNN regular vectorizando nuestras palabras y entrenando el modelo para realizar una prediccion\n",
    "Primero vamos a dejar todas nuestras palabras como texto, y eliminamos cualquier signo de puntuación"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cargamos todos  los paquetes necesarios para realizar nuestra RNN por LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hacemos de nuestras Keywords un solo texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "Key_Text= ', '.join(Key)\n",
    "processed_text = re.sub('[^a-zA-Z]',r' ', Key_Text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Le damos asignamos un valor a todos los signos del vocabulario que se encuentren en el texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{' ': 0,\n",
       " \"'\": 1,\n",
       " ',': 2,\n",
       " '0': 3,\n",
       " '1': 4,\n",
       " '2': 5,\n",
       " '3': 6,\n",
       " '4': 7,\n",
       " '5': 8,\n",
       " '6': 9,\n",
       " '7': 10,\n",
       " '9': 11,\n",
       " 'a': 12,\n",
       " 'b': 13,\n",
       " 'c': 14,\n",
       " 'd': 15,\n",
       " 'e': 16,\n",
       " 'f': 17,\n",
       " 'g': 18,\n",
       " 'h': 19,\n",
       " 'i': 20,\n",
       " 'j': 21,\n",
       " 'k': 22,\n",
       " 'l': 23,\n",
       " 'm': 24,\n",
       " 'n': 25,\n",
       " 'o': 26,\n",
       " 'p': 27,\n",
       " 'r': 28,\n",
       " 's': 29,\n",
       " 't': 30,\n",
       " 'u': 31,\n",
       " 'v': 32,\n",
       " 'w': 33,\n",
       " 'x': 34,\n",
       " 'y': 35,\n",
       " 'z': 36}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chars = sorted(list(set(Key_Text)))\n",
    "char_to_int = dict((c, i) for i, c in enumerate(chars))\n",
    "char_to_int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Characters:  2798\n",
      "Total Vocab:  37\n"
     ]
    }
   ],
   "source": [
    "n_chars = len(Key_Text)\n",
    "n_vocab = len(chars)\n",
    "print (\"Total Characters: \", n_chars)\n",
    "print (\"Total Vocab: \", n_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ahora convertimmos vectorizammos nuestras palabras por medio de la tokenización anterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Patterns:  2698\n"
     ]
    }
   ],
   "source": [
    "seq_length = 100\n",
    "dataX = []\n",
    "dataY = []\n",
    "for i in range(0, n_chars - seq_length, 1):\n",
    "    seq_in =  Key_Text[i:i + seq_length]\n",
    "    seq_out = Key_Text[i + seq_length]\n",
    "    dataX.append([char_to_int[char] for char in seq_in])\n",
    "    dataY.append(char_to_int[seq_out])\n",
    "n_patterns = len(dataX)\n",
    "print (\"Total Patterns: \", n_patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.reshape(dataX, (n_patterns, seq_length, 1))\n",
    "# normalize\n",
    "X = X / float(n_vocab)\n",
    "# one hot encode the output variable\n",
    "y = np_utils.to_categorical(dataY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Realizamos nuestra LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2]), return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(256))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Guardamos un checkpoint de los mejores modelos por epoca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory already exists\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    os.mkdir(\"checkpoint\")\n",
    "    print(\"Directory Created\") \n",
    "except FileExistsError:\n",
    "    print(\"Directory already exists\")\n",
    "\n",
    "filepath=\"checkpoint\\weights-improvement-{loss:.3f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Entrenamos nuestra RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "2698/2698 [==============================] - 27s 10ms/step - loss: 3.1325\n",
      "\n",
      "Epoch 00001: loss improved from inf to 3.13248, saving model to checkpoint\\weights-improvement-3.1325.hdf5\n",
      "Epoch 2/25\n",
      "2698/2698 [==============================] - 30s 11ms/step - loss: 2.9784\n",
      "\n",
      "Epoch 00002: loss improved from 3.13248 to 2.97843, saving model to checkpoint\\weights-improvement-2.9784.hdf5\n",
      "Epoch 3/25\n",
      "2698/2698 [==============================] - 33s 12ms/step - loss: 2.9675\n",
      "\n",
      "Epoch 00003: loss improved from 2.97843 to 2.96749, saving model to checkpoint\\weights-improvement-2.9675.hdf5\n",
      "Epoch 4/25\n",
      "2698/2698 [==============================] - 37s 14ms/step - loss: 2.9579\n",
      "\n",
      "Epoch 00004: loss improved from 2.96749 to 2.95791, saving model to checkpoint\\weights-improvement-2.9579.hdf5\n",
      "Epoch 5/25\n",
      "2698/2698 [==============================] - 39s 14ms/step - loss: 2.9662\n",
      "\n",
      "Epoch 00005: loss did not improve from 2.95791\n",
      "Epoch 6/25\n",
      "2698/2698 [==============================] - 40s 15ms/step - loss: 2.9617\n",
      "\n",
      "Epoch 00006: loss did not improve from 2.95791\n",
      "Epoch 7/25\n",
      "2698/2698 [==============================] - 41s 15ms/step - loss: 2.9649\n",
      "\n",
      "Epoch 00007: loss did not improve from 2.95791\n",
      "Epoch 8/25\n",
      "2698/2698 [==============================] - 42s 16ms/step - loss: 2.9456\n",
      "\n",
      "Epoch 00008: loss improved from 2.95791 to 2.94560, saving model to checkpoint\\weights-improvement-2.9456.hdf5\n",
      "Epoch 9/25\n",
      "2698/2698 [==============================] - 46s 17ms/step - loss: 2.9475\n",
      "\n",
      "Epoch 00009: loss did not improve from 2.94560\n",
      "Epoch 10/25\n",
      "2698/2698 [==============================] - 44s 16ms/step - loss: 2.9137\n",
      "\n",
      "Epoch 00010: loss improved from 2.94560 to 2.91372, saving model to checkpoint\\weights-improvement-2.9137.hdf5\n",
      "Epoch 11/25\n",
      "2698/2698 [==============================] - 46s 17ms/step - loss: 2.8695\n",
      "\n",
      "Epoch 00011: loss improved from 2.91372 to 2.86946, saving model to checkpoint\\weights-improvement-2.8695.hdf5\n",
      "Epoch 12/25\n",
      "2698/2698 [==============================] - 49s 18ms/step - loss: 2.8114\n",
      "\n",
      "Epoch 00012: loss improved from 2.86946 to 2.81142, saving model to checkpoint\\weights-improvement-2.8114.hdf5\n",
      "Epoch 13/25\n",
      "2698/2698 [==============================] - 42s 16ms/step - loss: 2.7603\n",
      "\n",
      "Epoch 00013: loss improved from 2.81142 to 2.76026, saving model to checkpoint\\weights-improvement-2.7603.hdf5\n",
      "Epoch 14/25\n",
      "2698/2698 [==============================] - 43s 16ms/step - loss: 2.7161\n",
      "\n",
      "Epoch 00014: loss improved from 2.76026 to 2.71614, saving model to checkpoint\\weights-improvement-2.7161.hdf5\n",
      "Epoch 15/25\n",
      "2698/2698 [==============================] - 43s 16ms/step - loss: 2.6714\n",
      "\n",
      "Epoch 00015: loss improved from 2.71614 to 2.67138, saving model to checkpoint\\weights-improvement-2.6714.hdf5\n",
      "Epoch 16/25\n",
      "2698/2698 [==============================] - 43s 16ms/step - loss: 2.6172\n",
      "\n",
      "Epoch 00016: loss improved from 2.67138 to 2.61718, saving model to checkpoint\\weights-improvement-2.6172.hdf5\n",
      "Epoch 17/25\n",
      "2698/2698 [==============================] - 44s 16ms/step - loss: 2.5458\n",
      "\n",
      "Epoch 00017: loss improved from 2.61718 to 2.54583, saving model to checkpoint\\weights-improvement-2.5458.hdf5\n",
      "Epoch 18/25\n",
      "2698/2698 [==============================] - 45s 16ms/step - loss: 2.4739\n",
      "\n",
      "Epoch 00018: loss improved from 2.54583 to 2.47393, saving model to checkpoint\\weights-improvement-2.4739.hdf5\n",
      "Epoch 19/25\n",
      "2698/2698 [==============================] - 45s 17ms/step - loss: 2.3979\n",
      "\n",
      "Epoch 00019: loss improved from 2.47393 to 2.39788, saving model to checkpoint\\weights-improvement-2.3979.hdf5\n",
      "Epoch 20/25\n",
      "2698/2698 [==============================] - 44s 16ms/step - loss: 2.2942\n",
      "\n",
      "Epoch 00020: loss improved from 2.39788 to 2.29420, saving model to checkpoint\\weights-improvement-2.2942.hdf5\n",
      "Epoch 21/25\n",
      "2698/2698 [==============================] - 45s 17ms/step - loss: 2.2132\n",
      "\n",
      "Epoch 00021: loss improved from 2.29420 to 2.21317, saving model to checkpoint\\weights-improvement-2.2132.hdf5\n",
      "Epoch 22/25\n",
      "2698/2698 [==============================] - 45s 17ms/step - loss: 2.1280\n",
      "\n",
      "Epoch 00022: loss improved from 2.21317 to 2.12800, saving model to checkpoint\\weights-improvement-2.1280.hdf5\n",
      "Epoch 23/25\n",
      "2698/2698 [==============================] - 48s 18ms/step - loss: 2.0131\n",
      "\n",
      "Epoch 00023: loss improved from 2.12800 to 2.01307, saving model to checkpoint\\weights-improvement-2.0131.hdf5\n",
      "Epoch 24/25\n",
      "2698/2698 [==============================] - 48s 18ms/step - loss: 1.9265\n",
      "\n",
      "Epoch 00024: loss improved from 2.01307 to 1.92654, saving model to checkpoint\\weights-improvement-1.9265.hdf5\n",
      "Epoch 25/25\n",
      "2698/2698 [==============================] - 48s 18ms/step - loss: 1.8630\n",
      "\n",
      "Epoch 00025: loss improved from 1.92654 to 1.86303, saving model to checkpoint\\weights-improvement-1.8630.hdf5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x215f0310048>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X, y, epochs=25, batch_size=128, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_modeled():\n",
    "    csv_files=list(filter(lambda x: '.hdf5' in x, os.listdir(\"checkpoint\")))\n",
    "    A = []\n",
    "    B =[]\n",
    "    C= np.zeros(len(csv_files))\n",
    "    for i in range(len(csv_files)):\n",
    "        A.append(csv_files[i].split('-'))\n",
    "        B.append(A[i][2].split('.h'))\n",
    "        C[i] = float(B[i][0])\n",
    "    mini = np.min(C)\n",
    "    return mini\n",
    "mini = get_best_modeled()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"checkpoint\\weights-improvement-\"+str(mini)+\".hdf5\"\n",
    "model.load_weights(filename)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Realizamos una prediccion de texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_to_ints = dict((i, c) for i, c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed:\n",
      "\" der armour sale, under armour jacket, under armour hoodies men's, under armour shorts women, under a \"\n",
      "rmour sooe\n",
      " Done.\n"
     ]
    }
   ],
   "source": [
    "start = np.random.randint(0, len(dataX)-1)\n",
    "pattern = dataX[start]\n",
    "print (\"Seed:\")\n",
    "print (\"\\\"\", ''.join([char_to_ints[value] for value in pattern]), \"\\\"\")\n",
    "# generate characters\n",
    "for i in range(10):\n",
    "    x = np.reshape(pattern, (1, len(pattern), 1))\n",
    "    x = x / float(n_vocab)\n",
    "    prediction = model.predict(x, verbose=0)\n",
    "    index = np.argmax(prediction)\n",
    "    result = char_to_ints[index]\n",
    "    seq_in = [char_to_ints[value] for value in pattern]\n",
    "    sys.stdout.write(result)\n",
    "    pattern.append(index)\n",
    "    pattern = pattern[1:len(pattern)]\n",
    "print (\"\\n Done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Caso 2\n",
    "Nuestra LSTM producirá un nuevo texto carácter por carácter. Tendremos que muestrear la letra correcta de las predicciones de la LSTM cada vez. La función de muestra acepta los siguientes dos parámetros:\n",
    "\n",
    ">* preds -> Las neuronas de salida.\n",
    ">* temperatura: 1.0 es el más conservador, 0.0 es el más arriesgado (dispuesto a cometer errores ortográficos y de otro tipo).\n",
    "\n",
    "La función realiza básicamente básicamenteun softmax en las predicciones de la red neuronal. Esto hace que cada neurona de salida se convierta en una probabilidad de su letra particular."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cargamos Paquete faltantes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import LambdaCallback\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras.utils import get_file\n",
    "import random\n",
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpus length: 2798\n",
      "total chars: 26\n"
     ]
    }
   ],
   "source": [
    "print('corpus length:', len(processed_text))\n",
    "\n",
    "chars = sorted(list(set(processed_text)))\n",
    "print('total chars:', len(chars))\n",
    "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
    "indices_char = dict((i, c) for i, c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Revisamos cuantas subsecuencias de palabras podemos generar de nuestra lista de Keywords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nb sequences: 926\n"
     ]
    }
   ],
   "source": [
    "maxlen = 20\n",
    "step = 3\n",
    "sentences = []\n",
    "next_chars = []\n",
    "for i in range(0, len(processed_text) - maxlen, step):\n",
    "    sentences.append(processed_text[i: i + maxlen])\n",
    "    next_chars.append(processed_text[i + maxlen])\n",
    "print('nb sequences:', len(sentences))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vectorizamos nuestras Keywords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorization...\n"
     ]
    }
   ],
   "source": [
    "print('Vectorization...')\n",
    "x = np.zeros((len(sentences), maxlen, len(chars)), dtype=np.bool)\n",
    "y = np.zeros((len(sentences), len(chars)), dtype=np.bool)\n",
    "for i, sentence in enumerate(sentences):\n",
    "    for t, char in enumerate(sentence):\n",
    "        x[i, t, char_indices[char]] = 1\n",
    "    y[i, char_indices[next_chars[i]]] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Construimos nuestro modelo LSTM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n"
     ]
    }
   ],
   "source": [
    "# build the model: a single LSTM\n",
    "print('Build model...')\n",
    "model = Sequential()\n",
    "model.add(LSTM(128, input_shape=(maxlen, len(chars))))\n",
    "model.add(Dense(len(chars), activation='softmax'))\n",
    "\n",
    "optimizer = RMSprop(lr=0.01)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Revisamos la arquitectura de nuestra LSTM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_3 (LSTM)                (None, 128)               79360     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 26)                3354      \n",
      "=================================================================\n",
      "Total params: 82,714\n",
      "Trainable params: 82,714\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Añadimos las variables preds y temperature\n",
    "Esto nos permite cambiar las probabilidades por cada letra desde un modo conservador hasta lo más arriesgado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(preds, temperature=1.0):\n",
    "    # helper function to sample an index from a probability array\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### La funcion on_epoch_end nos va a permitir imprimir una generacion de texto por epoca iterando sobre las temperaturas y los preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def on_epoch_end(epoch, _):\n",
    "    # Function invoked at end of each epoch. Prints generated text.\n",
    "    print(\"****************************************************************************\")\n",
    "    print('----- Generating text after Epoch: %d' % epoch)\n",
    "\n",
    "    start_index = random.randint(0, 30)\n",
    "    for temperature in [0.2, 0.5, 1.0]:\n",
    "        print('----- temperature:', temperature)\n",
    "\n",
    "        generated = ''\n",
    "        sentence = processed_text[start_index: start_index + maxlen]\n",
    "        generated += sentence\n",
    "        print('----- Generating with seed: \"' + sentence + '\"')\n",
    "        sys.stdout.write(generated)\n",
    "\n",
    "        for i in range(50):\n",
    "            x_pred = np.zeros((1, maxlen, len(chars)))\n",
    "            for t, char in enumerate(sentence):\n",
    "                x_pred[0, t, char_indices[char]] = 1.\n",
    "\n",
    "            preds = model.predict(x_pred, verbose=0)[0]\n",
    "            next_index = sample(preds, temperature)\n",
    "            next_char = indices_char[next_index]\n",
    "\n",
    "            generated += next_char\n",
    "            sentence = sentence[1:] + next_char\n",
    "\n",
    "            sys.stdout.write(next_char)\n",
    "            sys.stdout.flush()\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lamda Callback\n",
    "\n",
    "Un Callback puede realizar acciones en varias etapas del entrenamiento (por ejemplo, al comienzo o al final de una época, antes o después de un solo lote, etc.).\n",
    "\n",
    "Se pueden usar Callbacks para:\n",
    "\n",
    "* Escribir registros de TensorBoard después de cada lote de entrenamiento para monitorear sus métricas\n",
    "* Periódicamente guarde su modelo en el disco\n",
    "* Haz paradas tempranas\n",
    "* Obtener una vista de los estados internos y las estadísticas de un modelo durante el entrenamiento\n",
    "\n",
    "De esta forma lo podemos utilizar como:\n",
    "\n",
    ">tf.keras.callbacks.LambdaCallback(\n",
    "    on_epoch_begin=None, on_epoch_end=None, on_batch_begin=None, on_batch_end=None,\n",
    "    on_train_begin=None, on_train_end=None, **kwargs\n",
    ")\n",
    "\n",
    "**En este caso vamos a utilizar LambdaCallback para imprimir nuestra las predicciones hechas en la funcion on_epoch_end al final de cada entrenamiento de época**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "926/926 [==============================] - 1s 797us/step - loss: 3.1668\n",
      "****************************************************************************\n",
      "----- Generating text after Epoch: 0\n",
      "----- temperature: 0.2\n",
      "----- Generating with seed: \"hoes  nike outlet  n\"\n",
      "hoes  nike outlet  naaaaaaaasaasaasaaaaaaaaaaaaa aaa aaaaaaaaaaaaaaaaa\n",
      "----- temperature: 0.5\n",
      "----- Generating with seed: \"hoes  nike outlet  n\"\n",
      "hoes  nike outlet  nessaaaaa assasaaaa aaausaaaand asa a aada aaaaas a\n",
      "----- temperature: 1.0\n",
      "----- Generating with seed: \"hoes  nike outlet  n\"\n",
      "hoes  nike outlet  n ieshsa snvaoeasera  nsa cdisaa  pdsuso uapdsaa me\n",
      "Epoch 2/25\n",
      "926/926 [==============================] - 1s 569us/step - loss: 2.7395\n",
      "****************************************************************************\n",
      "----- Generating text after Epoch: 1\n",
      "----- temperature: 0.2\n",
      "----- Generating with seed: \"e shoes  nike outlet\"\n",
      "e shoes  nike outlet r   s  r  r       s           n         n        \n",
      "----- temperature: 0.5\n",
      "----- Generating with seed: \"e shoes  nike outlet\"\n",
      "e shoes  nike outletuossnusar  s    rc g ms     m ssss ror  r s sr sa \n",
      "----- temperature: 1.0\n",
      "----- Generating with seed: \"e shoes  nike outlet\"\n",
      "e shoes  nike outletessuarsuri stpusre ain ao  tg  ortn  gnjjlrpm brmp\n",
      "Epoch 3/25\n",
      "926/926 [==============================] - 1s 570us/step - loss: 2.6737\n",
      "****************************************************************************\n",
      "----- Generating text after Epoch: 2\n",
      "----- temperature: 0.2\n",
      "----- Generating with seed: \"e shoes  nike outlet\"\n",
      "e shoes  nike outlet                                                  \n",
      "----- temperature: 0.5\n",
      "----- Generating with seed: \"e shoes  nike outlet\"\n",
      "e shoes  nike outlet   a  a    t athsa      aae u     a sai   m ae   a\n",
      "----- temperature: 1.0\n",
      "----- Generating with seed: \"e shoes  nike outlet\"\n",
      "e shoes  nike outlet ve   art hmh  et  nad  l tps u ltt  ros  dtot a e\n",
      "Epoch 4/25\n",
      "926/926 [==============================] - 1s 577us/step - loss: 2.5974\n",
      "****************************************************************************\n",
      "----- Generating text after Epoch: 3\n",
      "----- temperature: 0.2\n",
      "----- Generating with seed: \"nike outlet  nike ho\"\n",
      "nike outlet  nike ho                                                  \n",
      "----- temperature: 0.5\n",
      "----- Generating with seed: \"nike outlet  nike ho\"\n",
      "nike outlet  nike ho   w ms    la     mhoe a  a o oa uoae s  s ua  pum\n",
      "----- temperature: 1.0\n",
      "----- Generating with seed: \"nike outlet  nike ho\"\n",
      "nike outlet  nike hoa jteooes n tuurnslemhhosorusma  o r al leimsvgeu \n",
      "Epoch 5/25\n",
      "926/926 [==============================] - 1s 564us/step - loss: 2.4578\n",
      "****************************************************************************\n",
      "----- Generating text after Epoch: 4\n",
      "----- temperature: 0.2\n",
      "----- Generating with seed: \"e outlet  nike hoodi\"\n",
      "e outlet  nike hoodida aaamamaaa aaaaaamaaamaaamaaamaaaaauamaamaaaaaaa\n",
      "----- temperature: 0.5\n",
      "----- Generating with seed: \"e outlet  nike hoodi\"\n",
      "e outlet  nike hoodia  ma aadaa aaaamarr amauammummaaa amadum a amaaaa\n",
      "----- temperature: 1.0\n",
      "----- Generating with seed: \"e outlet  nike hoodi\"\n",
      "e outlet  nike hoodiouuidr momeadaadama  mtmua umrak  m mar oaaammaoma\n",
      "Epoch 6/25\n",
      "926/926 [==============================] - 1s 562us/step - loss: 2.3093\n",
      "****************************************************************************\n",
      "----- Generating text after Epoch: 5\n",
      "----- temperature: 0.2\n",
      "----- Generating with seed: \"es  nike outlet  nik\"\n",
      "es  nike outlet  nike   eeis   ees   ooee    eee   ee    eoe   eee   o\n",
      "----- temperature: 0.5\n",
      "----- Generating with seed: \"es  nike outlet  nik\"\n",
      "es  nike outlet  nike   oedes  aeos    ees  leis  eadees  eone  eees  \n",
      "----- temperature: 1.0\n",
      "----- Generating with seed: \"es  nike outlet  nik\"\n",
      "es  nike outlet  niks oeidts  adn  iea moaestes houls haes arsoeaes   \n",
      "Epoch 7/25\n",
      "926/926 [==============================] - 1s 560us/step - loss: 2.1689\n",
      "****************************************************************************\n",
      "----- Generating text after Epoch: 6\n",
      "----- temperature: 0.2\n",
      "----- Generating with seed: \"nike shoes  nike out\"\n",
      "nike shoes  nike outs  anes  aat s ants aats  ant   aans  aat  aans  a\n",
      "----- temperature: 0.5\n",
      "----- Generating with seed: \"nike shoes  nike out\"\n",
      "nike shoes  nike outs untess aomers nas  aroas   ants  aages nans  ans\n",
      "----- temperature: 1.0\n",
      "----- Generating with seed: \"nike shoes  nike out\"\n",
      "nike shoes  nike outi saraje   eneriss  hnes rnne  adist aacthts na s \n",
      "Epoch 8/25\n",
      "926/926 [==============================] - 1s 578us/step - loss: 1.9919\n",
      "****************************************************************************\n",
      "----- Generating text after Epoch: 7\n",
      "----- temperature: 0.2\n",
      "----- Generating with seed: \"let  nike hoodie  ni\"\n",
      "let  nike hoodie  niras   aroor   aroors  paroor   aroors   aroors  ar\n",
      "----- temperature: 0.5\n",
      "----- Generating with seed: \"let  nike hoodie  ni\"\n",
      "let  nike hoodie  niras  aroir   nanr   aroor   rroor s  roors  es  oo\n",
      "----- temperature: 1.0\n",
      "----- Generating with seed: \"let  nike hoodie  ni\"\n",
      "let  nike hoodie  nier smocg la  araoirarr rooodc s urmoarar  vomra  a\n",
      "Epoch 9/25\n",
      "926/926 [==============================] - 1s 568us/step - loss: 1.8708\n",
      "****************************************************************************\n",
      "----- Generating text after Epoch: 8\n",
      "----- temperature: 0.2\n",
      "----- Generating with seed: \"s  nike outlet  nike\"\n",
      "s  nike outlet  nikes  unders s  nieas punders  ndert  under  under  r\n",
      "----- temperature: 0.5\n",
      "----- Generating with seed: \"s  nike outlet  nike\"\n",
      "s  nike outlet  niker lint  nnee   umets s neder sledts  ndees under a\n",
      "----- temperature: 1.0\n",
      "----- Generating with seed: \"s  nike outlet  nike\"\n",
      "s  nike outlet  nikes uidet  shteat    nder s shituts nrmorasour uns p\n",
      "Epoch 10/25\n",
      "926/926 [==============================] - 1s 556us/step - loss: 1.6696\n",
      "****************************************************************************\n",
      "----- Generating text after Epoch: 9\n",
      "----- temperature: 0.2\n",
      "----- Generating with seed: \"nike shoes  nike out\"\n",
      "nike shoes  nike outas aadidas aaalevis  araounder aarour  adeas aroou\n",
      "----- temperature: 0.5\n",
      "----- Generating with seed: \"nike shoes  nike out\"\n",
      "nike shoes  nike outsearounder aroour aadaraaoounder arat   anderaaa o\n",
      "----- temperature: 1.0\n",
      "----- Generating with seed: \"nike shoes  nike out\"\n",
      "nike shoes  nike outaes shevidar ajacasoode  roiigs  naraemor t nea aa\n",
      "Epoch 11/25\n",
      "926/926 [==============================] - 1s 556us/step - loss: 1.5411\n",
      "****************************************************************************\n",
      "----- Generating text after Epoch: 10\n",
      "----- temperature: 0.2\n",
      "----- Generating with seed: \"tlet  nike hoodie  n\"\n",
      "tlet  nike hoodie  nide  aroour  aroour  aroour  aroour    levis   adi\n",
      "----- temperature: 0.5\n",
      "----- Generating with seed: \"tlet  nike hoodie  n\"\n",
      "tlet  nike hoodie  nide  arioue  under aroour  thoes  aroole    uole  \n",
      "----- temperature: 1.0\n",
      "----- Generating with seed: \"tlet  nike hoodie  n\"\n",
      "tlet  nike hoodie  noole  aoooh s bariea  hoode sachoeu   levis  hooe \n",
      "Epoch 12/25\n",
      "926/926 [==============================] - 1s 553us/step - loss: 1.3715\n",
      "****************************************************************************\n",
      "----- Generating text after Epoch: 11\n",
      "----- temperature: 0.2\n",
      "----- Generating with seed: \"nike shoes  nike out\"\n",
      "nike shoes  nike outlers  aoolina  anider  aroour s   nike s ooolina  \n",
      "----- temperature: 0.5\n",
      "----- Generating with seed: \"nike shoes  nike out\"\n",
      "nike shoes  nike outlers levis    ooes  neis     oools spacts ciks  oo\n",
      "----- temperature: 1.0\n",
      "----- Generating with seed: \"nike shoes  nike out\"\n",
      "nike shoes  nike outs  ndirtshogtk rooidnr    oooide s oooin  shooon  \n",
      "Epoch 13/25\n",
      "926/926 [==============================] - 1s 583us/step - loss: 1.2091\n",
      "****************************************************************************\n",
      "----- Generating text after Epoch: 12\n",
      "----- temperature: 0.2\n",
      "----- Generating with seed: \"hoes  nike outlet  n\"\n",
      "hoes  nike outlet  nididas   under aroour s     levis  arolinas  nider\n",
      "----- temperature: 0.5\n",
      "----- Generating with seed: \"hoes  nike outlet  n\"\n",
      "hoes  nike outlet  nididas soooiss  nider sooois   noers  arooins     \n",
      "----- temperature: 1.0\n",
      "----- Generating with seed: \"hoes  nike outlet  n\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hoes  nike outlet  nigdes  rolinss bonde  armooss s  uidar aroouns ban\n",
      "Epoch 14/25\n",
      "926/926 [==============================] - 1s 577us/step - loss: 1.1217\n",
      "****************************************************************************\n",
      "----- Generating text after Epoch: 13\n",
      "----- temperature: 0.2\n",
      "----- Generating with seed: \" outlet  nike hoodie\"\n",
      " outlet  nike hoodie  ander armour  adidas  adidas  adidas panthers  u\n",
      "----- temperature: 0.5\n",
      "----- Generating with seed: \" outlet  nike hoodie\"\n",
      " outlet  nike hoodiea pundersaroour  anieas arolina pant ers  adidas p\n",
      "----- temperature: 1.0\n",
      "----- Generating with seed: \" outlet  nike hoodie\"\n",
      " outlet  nike hoodiens levis  ander sarts  mengse sns hondees hors  ro\n",
      "Epoch 15/25\n",
      "926/926 [==============================] - 1s 602us/step - loss: 1.0028\n",
      "****************************************************************************\n",
      "----- Generating text after Epoch: 14\n",
      "----- temperature: 0.2\n",
      "----- Generating with seed: \" nike outlet  nike h\"\n",
      " nike outlet  nike hoolie  arllec  arllev  arolina panthers  under arm\n",
      "----- temperature: 0.5\n",
      "----- Generating with seed: \" nike outlet  nike h\"\n",
      " nike outlet  nike hoolina puml  carolina pandlevs jen  arlea  ardlea \n",
      "----- temperature: 1.0\n",
      "----- Generating with seed: \" nike outlet  nike h\"\n",
      " nike outlet  nike horder hola  under aroluns lens sunler  under arthl\n",
      "Epoch 16/25\n",
      "926/926 [==============================] - 1s 559us/step - loss: 0.8657\n",
      "****************************************************************************\n",
      "----- Generating text after Epoch: 15\n",
      "----- temperature: 0.2\n",
      "----- Generating with seed: \"tlet  nike hoodie  n\"\n",
      "tlet  nike hoodie  nike woouiina pamthens  adidas armoun  adidas puman\n",
      "----- temperature: 0.5\n",
      "----- Generating with seed: \"tlet  nike hoodie  n\"\n",
      "tlet  nike hoodie  nike swoatshers  adidas womina pammoun  ander armou\n",
      "----- temperature: 1.0\n",
      "----- Generating with seed: \"tlet  nike hoodie  n\"\n",
      "tlet  nike hoodie  nnker armour padts lect  cide  leathiit cate   adho\n",
      "Epoch 17/25\n",
      "926/926 [==============================] - 1s 568us/step - loss: 0.8061\n",
      "****************************************************************************\n",
      "----- Generating text after Epoch: 16\n",
      "----- temperature: 0.2\n",
      "----- Generating with seed: \"  nike hoodie  nike \"\n",
      "  nike hoodie  nike shirt      arolina panthers  carolina panthers  ca\n",
      "----- temperature: 0.5\n",
      "----- Generating with seed: \"  nike hoodie  nike \"\n",
      "  nike hoodie  nike shirt     pacthirt   puma shiatshirt     arolina p\n",
      "----- temperature: 1.0\n",
      "----- Generating with seed: \"  nike hoodie  nike \"\n",
      "  nike hoodie  nike sherthir   weatshiri  liggshirtst s swsathirts  di\n",
      "Epoch 18/25\n",
      "926/926 [==============================] - 1s 547us/step - loss: 0.6893\n",
      "****************************************************************************\n",
      "----- Generating text after Epoch: 17\n",
      "----- temperature: 0.2\n",
      "----- Generating with seed: \"nike  nike shoes  ni\"\n",
      "nike  nike shoes  nike swoatshirt  nike swoats  under armour  under ar\n",
      "----- temperature: 0.5\n",
      "----- Generating with seed: \"nike  nike shoes  ni\"\n",
      "nike  nike shoes  nike swoatsuni  under armour sthers ccooun  nike swo\n",
      "----- temperature: 1.0\n",
      "----- Generating with seed: \"nike  nike shoes  ni\"\n",
      "nike  nike shoes  nike wumanshoes  under armour ets mund s  actluru cu\n",
      "Epoch 19/25\n",
      "926/926 [==============================] - 1s 549us/step - loss: 0.6372\n",
      "****************************************************************************\n",
      "----- Generating text after Epoch: 18\n",
      "----- temperature: 0.2\n",
      "----- Generating with seed: \"et  nike hoodie  nik\"\n",
      "et  nike hoodie  nike shoes fen  anidas pacts  nike shoes fen  anidas \n",
      "----- temperature: 0.5\n",
      "----- Generating with seed: \"et  nike hoodie  nik\"\n",
      "et  nike hoodie  nike  under armour hoodie  levis  anidas sacts  puma \n",
      "----- temperature: 1.0\n",
      "----- Generating with seed: \"et  nike hoodie  nik\"\n",
      "et  nike hoodie  nike sunar acadiuanher  acdarouunkermoudleggorgggrg b\n",
      "Epoch 20/25\n",
      "926/926 [==============================] - 1s 550us/step - loss: 0.5511\n",
      "****************************************************************************\n",
      "----- Generating text after Epoch: 19\n",
      "----- temperature: 0.2\n",
      "----- Generating with seed: \"e outlet  nike hoodi\"\n",
      "e outlet  nike hoodie  nder armour hoodie  nder armour hoodie  nike sw\n",
      "----- temperature: 0.5\n",
      "----- Generating with seed: \"e outlet  nike hoodi\"\n",
      "e outlet  nike hoodie  nike shoes ens  nike swoetsoodie  nike swoetsen\n",
      "----- temperature: 1.0\n",
      "----- Generating with seed: \"e outlet  nike hoodi\"\n",
      "e outlet  nike hoodien carolling shoer  adidastooue kers copum  caroou\n",
      "Epoch 21/25\n",
      "926/926 [==============================] - 1s 571us/step - loss: 0.4957\n",
      "****************************************************************************\n",
      "----- Generating text after Epoch: 20\n",
      "----- temperature: 0.2\n",
      "----- Generating with seed: \"nike outlet  nike ho\"\n",
      "nike outlet  nike hoodie  nike hoodie  nike hoodie  nike hoodie  nike \n",
      "----- temperature: 0.5\n",
      "----- Generating with seed: \"nike outlet  nike ho\"\n",
      "nike outlet  nike hoodie  nike hoodie  olle  adidas armour homes  ader\n",
      "----- temperature: 1.0\n",
      "----- Generating with seed: \"nike outlet  nike ho\"\n",
      "nike outlet  nike hoodie  niket ooole  underwarmour kens  radhors  aro\n",
      "Epoch 22/25\n",
      "926/926 [==============================] - 1s 591us/step - loss: 0.4273\n",
      "****************************************************************************\n",
      "----- Generating text after Epoch: 21\n",
      "----- temperature: 0.2\n",
      "----- Generating with seed: \"t  nike hoodie  nike\"\n",
      "t  nike hoodie  nike hoodie  nder  under armour hoodies  under armour \n",
      "----- temperature: 0.5\n",
      "----- Generating with seed: \"t  nike hoodie  nike\"\n",
      "t  nike hoodie  nike hoodie  nder  under armour hoodies  under armour \n",
      "----- temperature: 1.0\n",
      "----- Generating with seed: \"t  nike hoodie  nike\"\n",
      "t  nike hoodie  nike lordien  didas wemanss nurter  oodidas  levis  ho\n",
      "Epoch 23/25\n",
      "926/926 [==============================] - 1s 596us/step - loss: 0.4387\n",
      "****************************************************************************\n",
      "----- Generating text after Epoch: 22\n",
      "----- temperature: 0.2\n",
      "----- Generating with seed: \"outlet  nike hoodie \"\n",
      "outlet  nike hoodie  nike   under armour hoodies  under armour  under \n",
      "----- temperature: 0.5\n",
      "----- Generating with seed: \"outlet  nike hoodie \"\n",
      "outlet  nike hoodie  nike shoes fur  nike   under armour hoodies  unde\n",
      "----- temperature: 1.0\n",
      "----- Generating with seed: \"outlet  nike hoodie \"\n",
      "outlet  nike hoodie  nike s jens  bens  levis   jevis  nike sweatlinss\n",
      "Epoch 24/25\n",
      "926/926 [==============================] - ETA: 0s - loss: 0.365 - 1s 639us/step - loss: 0.3627\n",
      "****************************************************************************\n",
      "----- Generating text after Epoch: 23\n",
      "----- temperature: 0.2\n",
      "----- Generating with seed: \"ike shoes  nike outl\"\n",
      "ike shoes  nike outler  aroouik  arooui  aroour shoes  under armour sh\n",
      "----- temperature: 0.5\n",
      "----- Generating with seed: \"ike shoes  nike outl\"\n",
      "ike shoes  nike outler arolina panthers ceroui  arooui  levis  unkeroa\n",
      "----- temperature: 1.0\n",
      "----- Generating with seed: \"ike shoes  nike outl\"\n",
      "ike shoes  nike outler  puma luvn  javis levis  levis dents backetlear\n",
      "Epoch 25/25\n",
      "926/926 [==============================] - 1s 624us/step - loss: 0.2981\n",
      "****************************************************************************\n",
      "----- Generating text after Epoch: 24\n",
      "----- temperature: 0.2\n",
      "----- Generating with seed: \"ike outlet  nike hoo\"\n",
      "ike outlet  nike hoodies men  anders shorts  under armour sthers short\n",
      "----- temperature: 0.5\n",
      "----- Generating with seed: \"ike outlet  nike hoo\"\n",
      "ike outlet  nike hoodies  underwarmour hootsess shorts  under armour m\n",
      "----- temperature: 1.0\n",
      "----- Generating with seed: \"ike outlet  nike hoo\"\n",
      "ike outlet  nike hoodies jeaneans puma thors sume sagts  nike swoatsun\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x215874b5808>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# See https://github.com/tensorflow/tensorflow/issues/31308\n",
    "import logging, os\n",
    "logging.disable(logging.WARNING)\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
    "\n",
    "# Fit the model\n",
    "print_callback = LambdaCallback(on_epoch_end=on_epoch_end)\n",
    "\n",
    "model.fit(x, y,\n",
    "          batch_size=128,\n",
    "          epochs=25,\n",
    "          callbacks=[print_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retos:\n",
    " ### Añadir Money Keywords,\n",
    " Son aquellas que no solo atraen clientes potenciales directamente al sitio, sino que también atraen a los clientes potenciales correctos a su sitio: los clientes que sacarán su tarjeta de crédito y  pagarán dinero.\n",
    " \n",
    "**Por ejemplo**\n",
    "\n",
    " Oferta, Préstamos, Hipoteca, Crédito, Hospedaje y  Reclamación\n",
    " \n",
    " ### Implementar el Keyword Research en un sitio web\n",
    " Al momento de implementar las palabras generadas en un sitio web, podremos observar y cuantizar sus resultados en los buscadores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusión\n",
    "\n",
    "El proyecto de Keyword  Research nos permitió conocer cual es la verdadera labor del SEO y su importancia en el Marketing Digital para sitios web, es un gran ejercicio para aprender sobre scrapping con Selenium, manejar archivos del sistema e incluso aprender sobre el verdadero significado de Marketing y varias de sus estrategias para lograr acercar lo mejor posible servicios y sitios web a clientes potenciales.\n",
    "\n",
    "Realizar KeyWord Research a traves de Python y con ayuda de Machine Learning, es un campo con muchas ideas por explorar y poco desarrolladas, y que con más trabajo se pueden realizar algoritmos muy eficientes para un posicionamiento Web orgánico.\n",
    "\n",
    "Todos estos proyectos nos permiten conocer nuevos algoritmos, trabajarlos y entenderlos de la mejor forma, además, estamos aprendiendo sobre el contexto en que podemos aplicar estos, entender las caracteristicas de las diferentes formas de aplicar Machine Learning en varias temáticas de todo tipo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
