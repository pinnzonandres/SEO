{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduccion "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ¿Qué es el SEO?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El posicionamiento en buscadores u optimización de motores de búsqueda es el proceso de mejorar la visibilidad de un sitio web en los resultados orgánicos (los que no son pagados), de los diferentes buscadores. También es frecuente nombrarlo por su título inglés, SEO (Search Engine Optimization)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/ejemplo-busqueda-seo2.png\" width=\"800\" height=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ¿Por qué es el SEO importante?\n",
    "La razón más importante por la que es necesario el SEO es porque hace más útil tu página web tanto para los usuarios como para los motores de búsqueda. Aunque estos aún no pueden ver una página web como lo hace un humano. El SEO es necesario para ayudar a los motores de búsqueda a entender sobre qué trata cada página y si es o no útil para los usuarios."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Codigo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definimos las funciones Iniciales\n",
    "Vamos a definir las funciones iniciales para el Keyword research de nuestro proyecto."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funcion para definir el idioma y la palabra clave a utilizar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_values():\n",
    "    print(\"Language:\")\n",
    "    print(\"(1) Español\")\n",
    "    print(\"(2) English\")\n",
    "    language = input()\n",
    "    if int(language) == 1:\n",
    "        print (\"Cuál es su Keyword?\")\n",
    "        Keyword= input()\n",
    "        LANG = \"es\"\n",
    "        CONT=\"CO\"\n",
    "        return Keyword, LANG, CONT\n",
    "    if int(language) == 2:\n",
    "        print(\"What is your keyword?\")\n",
    "        Keyword= input()\n",
    "        LANG = \"en\"\n",
    "        CONT=\"US\"\n",
    "        return Keyword, LANG, CONT\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funciones para escoger una nueva palabra y realizar todo el filtro."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 557,
   "metadata": {},
   "outputs": [],
   "source": [
    "def news():\n",
    "    if CONT == 'US':\n",
    "        print('Do you want to add another Keyword?')\n",
    "        print('(1) Yes')\n",
    "        print('(2) No')\n",
    "        choice = input()\n",
    "    if CONT == 'CO':\n",
    "        print('¿Desea añadir otra Keyword?')\n",
    "        print('(1) Si')\n",
    "        print('(2) No')\n",
    "        choice = input()\n",
    "    return choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 560,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_keyword():\n",
    "    a,b,c = set_values()\n",
    "    erasecsvdata()\n",
    "    downdata(a, b, c)\n",
    "    data = reading_data(a)\n",
    "    data = clean_zeros(data)\n",
    "    keywording = Keymarcas(data)\n",
    "    print('{} Keywords'.format(len(keywording)))\n",
    "    getalldata(keywording,a)\n",
    "    dates= rec_data(a)\n",
    "    data = pd.concat([ keywords_data, dates])\n",
    "    Key = keys(data)\n",
    "    Key = textkey(Key)\n",
    "    return Key,data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Función para eliminar datos con Keywords anteriores o innecesarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 575,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "path = Path.cwd()\n",
    "import shutil, os, sys\n",
    "\n",
    "def erasecsvdata():\n",
    "    csv_files=list(filter(lambda x: '.csv' in x, os.listdir(path)))\n",
    "    if CONT == 'CO':\n",
    "        print ('Estos son los CSV en tu carpeta')\n",
    "        for i in range(len(csv_files)):\n",
    "            print ('({}) -> {}'.format(i+1, csv_files[i]))\n",
    "        print ('Escoge los numeros de los CSV que quieres eliminar o 0 para pasar (ejemplo 1,3)')\n",
    "        numero = input()\n",
    "    if CONT == 'US':\n",
    "        print('This are the CSV on your folder')\n",
    "        for i in range(len(csv_files)):\n",
    "            print('({} -> {})'.format(i+1, csv_files[i]))\n",
    "        print('Choose the numbers of the CSV that you want to delete or choose 0 to pass (example 1,3)')\n",
    "        numero = input()\n",
    "    A = numero.split(',')\n",
    "    A = [int(integer) for integer in A]\n",
    "    for j in range(len(csv_files)):\n",
    "        #print(csv_files[j])\n",
    "        if j+1 in A:\n",
    "            os.remove(csv_files[j])\n",
    "    print('Los archivos restantes son:')\n",
    "    print (list(filter(lambda x: '.csv' in x, os.listdir(path))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funciones para descarga de datos\n",
    "Luego de definir nuestra Keyword objetivo, creamos las funciones para realizar la busqueda."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Función para descargar los datos de la Keyword Principal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 582,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver import Chrome\n",
    "from selenium.webdriver.support.ui import Select\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "\n",
    "import time\n",
    "\n",
    "\n",
    "def downdata(Keyword,LANG, CONT):\n",
    "    if os.path.exists( Keyword +'.csv') == False:\n",
    "        options = webdriver.ChromeOptions()\n",
    "        prefs = {\"download.default_directory\": str(path)}\n",
    "        options.add_experimental_option(\"prefs\", prefs)\n",
    "        browser = webdriver.Chrome(executable_path='chromedriver', options = options) \n",
    "        browser.get('https://cocolyze.com/en/google-keyword-planner-tool#null')\n",
    "    \n",
    "        username = browser.find_element_by_id('keyword')\n",
    "        username.send_keys(Keyword)\n",
    "\n",
    "\n",
    "        select_Country = Select(browser.find_element_by_id('country'))\n",
    "        select_Country.select_by_value(CONT)\n",
    "\n",
    "        select_lang = Select(browser.find_element_by_id(\"lang\"))\n",
    "        select_lang.select_by_value(LANG)\n",
    "\n",
    "        button = browser.find_element_by_id('submitBtn')\n",
    "        button.click()\n",
    "\n",
    "\n",
    "        time.sleep(15)\n",
    "\n",
    "        ids = browser.find_elements_by_xpath(\"//*[@href]\")\n",
    "        for ii in ids:\n",
    "            #print (ii.text)\n",
    "            if ii.text == 'CSV':\n",
    "                download = ii\n",
    "        #print (download.text)\n",
    "        download.click()\n",
    "\n",
    "\n",
    "        time.sleep(5)\n",
    "        browser.close()\n",
    "        os.rename('suggestion.csv', Keyword+'.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Función para descargar los datos para todos los keywords de una lista."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 542,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getalldata(Keywording,Keyword):\n",
    "    dire = \"Data_\"+ Keyword\n",
    "    try:\n",
    "    # Create target Directory\n",
    "        os.mkdir(dire)\n",
    "        print(\"Directory Created\") \n",
    "    except FileExistsError:\n",
    "        print(\"Directory already exists\")\n",
    "    options = webdriver.ChromeOptions()\n",
    "    prefs = {\"download.default_directory\": str(path)}\n",
    "    options.add_experimental_option(\"prefs\", prefs)\n",
    "    browser = webdriver.Chrome(executable_path='chromedriver', options = options) \n",
    "    browser.get('https://cocolyze.com/en/google-keyword-planner-tool#null')\n",
    "    for i in range(len(Keywording)):\n",
    "        if os.path.exists('Data_'+ Keyword +'/'+ Keywording[i]+str(i+1) +'.csv'):\n",
    "            continue\n",
    "        elif os.path.exists('Data_'+ Keyword+'/'+ Keywording[i] +'.csv')== False :\n",
    "            time.sleep(1)\n",
    "            username = browser.find_element_by_id('keyword')\n",
    "            username.clear()\n",
    "            username.send_keys(Keywording[i])\n",
    "\n",
    "            select_Country = Select(browser.find_element_by_id('country'))\n",
    "            select_Country.select_by_value(CONT)\n",
    "            select_lang = Select(browser.find_element_by_id(\"lang\"))\n",
    "            select_lang.select_by_value(LANG)\n",
    "            button = browser.find_element_by_id('submitBtn')\n",
    "            button.click()\n",
    "            time.sleep(15)\n",
    "            ids = browser.find_elements_by_xpath(\"//*[@href]\")\n",
    "            for ii in ids:\n",
    "                #print (ii.text)\n",
    "                if ii.text == 'CSV':\n",
    "                    download = ii\n",
    "            #print (download.text)\n",
    "            download.click()\n",
    "\n",
    "            time.sleep(3)\n",
    "  \n",
    "            os.rename('suggestion.csv',  Keywording[i]+str(i+1)+'.csv')\n",
    "    \n",
    "            shutil.move(Keywording[i]+str(i+1)+'.csv', str(path)+\"\\Data_\"+ Keyword)\n",
    "            time.sleep(2)\n",
    "    print(\"All data downloaded\")\n",
    "    browser.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entramos los valores y Keyword para nuestro SEO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 536,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Language:\n",
      "(1) Español\n",
      "(2) English\n",
      "2\n",
      "What is your keyword?\n",
      "Pants\n"
     ]
    }
   ],
   "source": [
    "Keyword, LANG, CONT = set_values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Borramos archivos innecesarios de alguna keyword erronea."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This are the CSV on your folder\n",
      "Choose the numbers of the CSV that you want to delete or choose 0 to pass (example 1,3)\n",
      "0\n",
      "Los archivos restantes son:\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "erasecsvdata()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Descargamos los datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "downdata(Keyword,LANG, CONT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lectura y limpieza de datos\n",
    "Ahora vamos a leer y limpiar el dataset descargado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Leemos el dataset bajo las columnas que necesitamos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   Keyword  Search Volume   CPC\n",
      "0           carolina pants         823000  1.11\n",
      "1  sponge bob square pants         246000  0.94\n",
      "2              sweat pants         165000  1.06\n",
      "3               pants yoga         165000  1.27\n",
      "4               yoga pants         165000  1.83\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "def reading_data(Keyword):\n",
    "    col_list = [\"Keyword\",\"Search Volume\",\"CPC\"]\n",
    "    keyword_Data=pd.read_csv(Keyword+'.csv', sep =\";\",usecols=col_list)\n",
    "    print(keyword_Data.head())\n",
    "    return keyword_Data\n",
    "keyword_Data = reading_data(Keyword)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Revisamos si existen datos vacios y eliminamos las filas con datos iguales a cero.\n",
    "Quitamos todos los datos con algún valor a cero ya que esto significa que no hay informacion acerca del volumen de busqueda o del costo por clic para la palabra correspondiente. Además, en caso que eliminemos más del 40% de la informacion, lanzamos una advertencia ya que es una perdida muy grande de información."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Null Data in Keyword = 0\n",
      "\n",
      "\n",
      "Null Data in Search Volume = 0\n",
      "\n",
      "\n",
      "Null Data in CPC = 0\n",
      "\n",
      "\n",
      "Great Dataset! Continue\n"
     ]
    }
   ],
   "source": [
    "def clean_zeros(keyword_Data):\n",
    "    col_names= keyword_Data.columns.tolist()\n",
    "    for column in col_names:\n",
    "        print(\"Null Data in {} = {}\".format(column,keyword_Data[column].isnull().sum()))\n",
    "        print(\"\\n\")\n",
    "    Total = len(keyword_Data)\n",
    "    for column in col_names[1:-1]:\n",
    "         fixed = keyword_Data.loc[keyword_Data[column] > 0]\n",
    "            \n",
    "    Ft= len(fixed)\n",
    "    rang = (Ft/Total)*100\n",
    "   \n",
    "    if CONT == 'CO':\n",
    "        if rang < 60:\n",
    "            print ('Se presenta más del 40% de datos sucios, se recomienda usar otra palabra')\n",
    "        else :\n",
    "            print('Tienes un buen dataset, continua')\n",
    "    if CONT == 'US': \n",
    "        if rang < 60:\n",
    "            print('The Data has 40% of Null data, you should use another Keyword')\n",
    "        else:\n",
    "            print('Great Dataset! Continue')\n",
    "    \n",
    "    return fixed\n",
    "\n",
    "\n",
    "\n",
    "keyword_Data = clean_zeros(keyword_Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Keyword</th>\n",
       "      <th>Search Volume</th>\n",
       "      <th>CPC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>carolina pants</td>\n",
       "      <td>823000</td>\n",
       "      <td>1.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sponge bob square pants</td>\n",
       "      <td>246000</td>\n",
       "      <td>0.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sweat pants</td>\n",
       "      <td>165000</td>\n",
       "      <td>1.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>pants yoga</td>\n",
       "      <td>165000</td>\n",
       "      <td>1.27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>yoga pants</td>\n",
       "      <td>165000</td>\n",
       "      <td>1.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>pants</td>\n",
       "      <td>135000</td>\n",
       "      <td>1.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>nike sweat pants</td>\n",
       "      <td>135000</td>\n",
       "      <td>0.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>men's jeans</td>\n",
       "      <td>135000</td>\n",
       "      <td>2.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>shirt</td>\n",
       "      <td>135000</td>\n",
       "      <td>0.86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>tops</td>\n",
       "      <td>135000</td>\n",
       "      <td>0.97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>pants cargo</td>\n",
       "      <td>110000</td>\n",
       "      <td>0.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>cargo pants</td>\n",
       "      <td>110000</td>\n",
       "      <td>0.97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>khaki pants</td>\n",
       "      <td>90500</td>\n",
       "      <td>1.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>the sisterhood of the traveling pants</td>\n",
       "      <td>90500</td>\n",
       "      <td>0.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>pants dickies</td>\n",
       "      <td>74000</td>\n",
       "      <td>0.87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>dickie pants</td>\n",
       "      <td>74000</td>\n",
       "      <td>0.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>khaki</td>\n",
       "      <td>74000</td>\n",
       "      <td>1.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>women's suit pants</td>\n",
       "      <td>60500</td>\n",
       "      <td>0.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>pants suit womens</td>\n",
       "      <td>60500</td>\n",
       "      <td>0.93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>sweater pants womens</td>\n",
       "      <td>60500</td>\n",
       "      <td>1.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>cargo pants womens</td>\n",
       "      <td>60500</td>\n",
       "      <td>0.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>plaid pants</td>\n",
       "      <td>60500</td>\n",
       "      <td>0.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>women&amp;#39;s cargo pants</td>\n",
       "      <td>60500</td>\n",
       "      <td>0.85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>women's pants suits</td>\n",
       "      <td>60500</td>\n",
       "      <td>0.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>men's sweat pants</td>\n",
       "      <td>60500</td>\n",
       "      <td>2.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>palazzo pants</td>\n",
       "      <td>60500</td>\n",
       "      <td>0.87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>women cargo pants</td>\n",
       "      <td>60500</td>\n",
       "      <td>0.97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>sisterhood of the travelling pants</td>\n",
       "      <td>60500</td>\n",
       "      <td>0.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>suit pants women's</td>\n",
       "      <td>60500</td>\n",
       "      <td>0.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>cargo pants for womens</td>\n",
       "      <td>60500</td>\n",
       "      <td>0.95</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  Keyword  Search Volume   CPC\n",
       "0                          carolina pants         823000  1.11\n",
       "1                 sponge bob square pants         246000  0.94\n",
       "2                             sweat pants         165000  1.06\n",
       "3                              pants yoga         165000  1.27\n",
       "4                              yoga pants         165000  1.83\n",
       "5                                   pants         135000  1.04\n",
       "6                        nike sweat pants         135000  0.49\n",
       "7                             men's jeans         135000  2.12\n",
       "8                                   shirt         135000  0.86\n",
       "9                                    tops         135000  0.97\n",
       "10                            pants cargo         110000  0.99\n",
       "11                            cargo pants         110000  0.97\n",
       "12                            khaki pants          90500  1.54\n",
       "13  the sisterhood of the traveling pants          90500  0.22\n",
       "14                          pants dickies          74000  0.87\n",
       "15                           dickie pants          74000  0.77\n",
       "16                                  khaki          74000  1.26\n",
       "17                     women's suit pants          60500  0.80\n",
       "18                      pants suit womens          60500  0.93\n",
       "19                   sweater pants womens          60500  1.12\n",
       "20                     cargo pants womens          60500  0.92\n",
       "21                            plaid pants          60500  0.74\n",
       "22                women&#39;s cargo pants          60500  0.85\n",
       "23                    women's pants suits          60500  0.96\n",
       "24                      men's sweat pants          60500  2.04\n",
       "25                          palazzo pants          60500  0.87\n",
       "26                      women cargo pants          60500  0.97\n",
       "27     sisterhood of the travelling pants          60500  0.12\n",
       "28                     suit pants women's          60500  0.96\n",
       "29                 cargo pants for womens          60500  0.95"
      ]
     },
     "execution_count": 304,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keyword_Data.head(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hacemos un filtro sobre las marcas que queremos analizar.\n",
    "Queremos filtrar nuestro dataset respecto a las palabras que estan relacionadas con los productos o marcas que queremos \"ofrecer\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "Marcas=['Nike', 'Adidas','Puma','Nautica','Levis', 'Under Armour', 'Zara', 'Diadora', 'Carolina']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **FUZZY WUZZY**\n",
    "FuzzyWuzzy es una biblioteca de Python que se utiliza para la coincidencia de cadenas de texto. La coincidencia de cadenas difusa es el proceso de encontrar cadenas que coinciden con un patrón dado. Básicamente utiliza la distancia de **Levenshtein** para calcular las diferencias entre secuencias.\n",
    "\n",
    "\n",
    "### Distancia de Levenshtein\n",
    "La distancia de Levenshtein es una métrica para medir la diferencia entre dos secuencias. Informalmente, la distancia de Levenshtein entre dos palabras es el número mínimo de ediciones de un solo carácter (es decir, inserciones, eliminaciones o sustituciones) requeridas para cambiar una palabra en la otra."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fuzzywuzzy import process, fuzz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_matches(query, choices, limit=30):\n",
    "    results = process.extract(query, choices, limit = limit, scorer = fuzz.partial_ratio)\n",
    "    return results "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hacemos un filtro sobre nuestro conjunto de palabras con aquellas que tengan mas de  un 90% de coincidencia con nuestras marcas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Keymarcas(keyword_Data):\n",
    "    keywords=[]\n",
    "    for marca in Marcas:\n",
    "        data = get_matches(marca,keyword_Data.Keyword)\n",
    "        for i in range(len(data)):\n",
    "            if data[i][1] > 90:\n",
    "                keywords.append(data[i][0]) \n",
    "    for i in range(len(keywords)):\n",
    "        keywords[i]=keywords[i].replace('.','')\n",
    "    return keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords = Keymarcas(keyword_Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['nike sweat pants',\n",
       " 'nike womens sweat pants',\n",
       " 'nike sweat pants for men',\n",
       " \"nike men's sweat pants\",\n",
       " \"men's nike pants\",\n",
       " 'nike pants mens',\n",
       " 'nike pants for men',\n",
       " 'nike mens pants',\n",
       " 'track pants nike',\n",
       " 'nike track pants',\n",
       " 'nike fleece tech pants',\n",
       " \"nike women's pants\",\n",
       " \"women's nike pants\",\n",
       " \"nike pants women's\",\n",
       " 'nike gold pants',\n",
       " 'nikelab womens pants',\n",
       " 'nike tech fleece pants',\n",
       " 'nike pants tech fleece',\n",
       " 'adidas pants',\n",
       " 'adidas tracksuit pants',\n",
       " 'tracksuit pants adidas',\n",
       " 'track pants adidas',\n",
       " 'adidas pants womens',\n",
       " 'adidas women pants',\n",
       " 'adidas pants for men',\n",
       " \"women's adidas pants\",\n",
       " \"adidas men's pants\",\n",
       " 'men adidas pants',\n",
       " 'adidas tiro 17 pants',\n",
       " 'adidas pants red',\n",
       " 'adidas pants soccer',\n",
       " 'adidas pants tiro 17',\n",
       " 'adidas soccer pants',\n",
       " 'adidas red pants',\n",
       " 'under armor pants',\n",
       " 'underarmour sweat pants',\n",
       " 'carolina pants']"
      ]
     },
     "execution_count": 310,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keywords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Para cada palabra encontrada, volvemos a realizar un Keyword Research"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 543,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory already exists\n",
      "All data downloaded\n"
     ]
    }
   ],
   "source": [
    "getalldata(keywords ,Keyword)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Leemos y concatenamos todas las palabras encontradas por cada Keyword de nuestra lista filtrada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rec_data(Keyword):\n",
    "    path_2 = 'Data_'+Keyword\n",
    "    files = [file for file in os.listdir(path_2) if not file.startswith('.')] # Ignore hidden files\n",
    "\n",
    "    keywords_data = pd.DataFrame()\n",
    "\n",
    "    for file in files:\n",
    "        current_data = pd.read_csv(path_2+\"/\"+file, sep =\";\", usecols=col_list)\n",
    "        keywords_data = pd.concat([keywords_data, current_data])\n",
    "    for  i in range(len(keywords)):\n",
    "        keywords_data= pd.concat([keywords_data,keyword_Data.loc[keyword_Data['Keyword']==keywords[2]]])\n",
    "    \n",
    "    keywords_data.to_csv(\"all_data_\"+Keyword+\".csv\", index=False)\n",
    "    return keywords_data\n",
    "keywords_data = rec_data(Keyword)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Keyword</th>\n",
       "      <th>Search Volume</th>\n",
       "      <th>CPC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>nike</td>\n",
       "      <td>4090000</td>\n",
       "      <td>0.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>adidas</td>\n",
       "      <td>3350000</td>\n",
       "      <td>0.13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>adidas</td>\n",
       "      <td>3350000</td>\n",
       "      <td>0.13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>under armour</td>\n",
       "      <td>1220000</td>\n",
       "      <td>0.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>under armour</td>\n",
       "      <td>1220000</td>\n",
       "      <td>0.15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Keyword  Search Volume   CPC\n",
       "0          nike        4090000  0.08\n",
       "1        adidas        3350000  0.13\n",
       "2        adidas        3350000  0.13\n",
       "3  under armour        1220000  0.15\n",
       "4  under armour        1220000  0.15"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keywords_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Eliminamos todas las palabras que tengan un volumen de busqueda menor al promedio y aquellas que esten repetidas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 583,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keys(keywords_data):\n",
    "    keywords_data = keywords_data.sort_values(by='Search Volume', ascending=False)\n",
    "    keywords_data= keywords_data.reset_index(drop=True)\n",
    "\n",
    "    keywords_data= keywords_data.loc[keywords_data['Search Volume']> keywords_data['Search Volume'].mean()]\n",
    "\n",
    "    Key= keywords_data['Keyword'].values.tolist()\n",
    "    Key = list(dict.fromkeys(Key))\n",
    "    return Key\n",
    "Key = keys(keywords_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extraemos las palabras como texto y eliminamos cualquier mayuscula para un menor procesamiento de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 586,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['nike',\n",
       " 'nfl schedule',\n",
       " 'adidas',\n",
       " 'nfl news',\n",
       " 'under armour',\n",
       " 'nike shoes',\n",
       " 'carolina pants',\n",
       " 'patriots schedule',\n",
       " 'panthers',\n",
       " 'nike outlet',\n",
       " 'adidas shoes',\n",
       " 'nfl shop',\n",
       " 'saints schedule',\n",
       " 'nike shoes for men',\n",
       " 'under armour outlet',\n",
       " 'nike backpacks',\n",
       " 'sweatpants',\n",
       " 'nike air max womens',\n",
       " 'adidas womens shoes',\n",
       " 'nike sweatpants',\n",
       " 'under armour shoes',\n",
       " 'nike sweat pants',\n",
       " 'nike shorts',\n",
       " 'adidas shoes for men',\n",
       " 'adidas outlet',\n",
       " 'joggers for men',\n",
       " 'nike windbreaker',\n",
       " \"men's joggers\",\n",
       " 'nike women',\n",
       " 'nike headbands']"
      ]
     },
     "execution_count": 586,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def textkey(Key):\n",
    "    Key = [text.lower() for text in Key]\n",
    "    return Key\n",
    "Key = textkey(Key)\n",
    "Key[:30]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Insertar otra Keyword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 564,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Do you want to add another Keyword?\n",
      "(1) Yes\n",
      "(2) No\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    choice = news()\n",
    "    if int(choice) == 1:\n",
    "        Key,data = new_keyword()\n",
    "    else:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 585,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['nike',\n",
       " 'nfl schedule',\n",
       " 'adidas',\n",
       " 'nfl news',\n",
       " 'under armour',\n",
       " 'nike shoes',\n",
       " 'carolina pants',\n",
       " 'patriots schedule',\n",
       " 'panthers',\n",
       " 'nike outlet',\n",
       " 'adidas shoes',\n",
       " 'nfl shop',\n",
       " 'saints schedule',\n",
       " 'nike shoes for men',\n",
       " 'under armour outlet',\n",
       " 'nike backpacks',\n",
       " 'sweatpants',\n",
       " 'nike air max womens',\n",
       " 'adidas womens shoes',\n",
       " 'nike sweatpants',\n",
       " 'under armour shoes',\n",
       " 'nike sweat pants',\n",
       " 'nike shorts',\n",
       " 'adidas shoes for men',\n",
       " 'adidas outlet',\n",
       " 'joggers for men',\n",
       " 'nike windbreaker',\n",
       " \"men's joggers\",\n",
       " 'nike women',\n",
       " 'nike headbands']"
      ]
     },
     "execution_count": 585,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Key[:30]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generando textos por medio LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Caso 1\n",
    "Vamos a realizar una RNN regular vectorizando nuestras palabras y entrenando el modelo para realizar una prediccion\n",
    "Primero vamos a dejar todas nuestras palabras como texto, y eliminamos cualquier signo de puntuación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 566,
   "metadata": {},
   "outputs": [],
   "source": [
    "Key_Text= ', '.join(Key)\n",
    "processed_text = re.sub('[^a-zA-Z]',r' ', Key_Text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Le damos asignamos un valor a todos los signos del vocabulario que se encuentren en el texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 567,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{' ': 0,\n",
       " '#': 1,\n",
       " '&': 2,\n",
       " \"'\": 3,\n",
       " '(': 4,\n",
       " ')': 5,\n",
       " ',': 6,\n",
       " '-': 7,\n",
       " '.': 8,\n",
       " '0': 9,\n",
       " '1': 10,\n",
       " '2': 11,\n",
       " '3': 12,\n",
       " '4': 13,\n",
       " '5': 14,\n",
       " '6': 15,\n",
       " '7': 16,\n",
       " '9': 17,\n",
       " ';': 18,\n",
       " 'a': 19,\n",
       " 'b': 20,\n",
       " 'c': 21,\n",
       " 'd': 22,\n",
       " 'e': 23,\n",
       " 'f': 24,\n",
       " 'g': 25,\n",
       " 'h': 26,\n",
       " 'i': 27,\n",
       " 'j': 28,\n",
       " 'k': 29,\n",
       " 'l': 30,\n",
       " 'm': 31,\n",
       " 'n': 32,\n",
       " 'o': 33,\n",
       " 'p': 34,\n",
       " 'q': 35,\n",
       " 'r': 36,\n",
       " 's': 37,\n",
       " 't': 38,\n",
       " 'u': 39,\n",
       " 'v': 40,\n",
       " 'w': 41,\n",
       " 'x': 42,\n",
       " 'y': 43,\n",
       " 'z': 44}"
      ]
     },
     "execution_count": 567,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chars = sorted(list(set(Key_Text)))\n",
    "char_to_int = dict((c, i) for i, c in enumerate(chars))\n",
    "char_to_int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 568,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Characters:  22643\n",
      "Total Vocab:  45\n"
     ]
    }
   ],
   "source": [
    "n_chars = len(Key_Text)\n",
    "n_vocab = len(chars)\n",
    "print (\"Total Characters: \", n_chars)\n",
    "print (\"Total Vocab: \", n_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ahora convertimmos nuestras palabras a formato int por medio de la tokenización anterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 569,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Patterns:  22543\n"
     ]
    }
   ],
   "source": [
    "seq_length = 100\n",
    "dataX = []\n",
    "dataY = []\n",
    "for i in range(0, n_chars - seq_length, 1):\n",
    "    seq_in =  Key_Text[i:i + seq_length]\n",
    "    seq_out = Key_Text[i + seq_length]\n",
    "    dataX.append([char_to_int[char] for char in seq_in])\n",
    "    dataY.append(char_to_int[seq_out])\n",
    "n_patterns = len(dataX)\n",
    "print (\"Total Patterns: \", n_patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = numpy.reshape(dataX, (n_patterns, seq_length, 1))\n",
    "# normalize\n",
    "X = X / float(n_vocab)\n",
    "# one hot encode the output variable\n",
    "y = np_utils.to_categorical(dataY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2]), return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(256))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath=\"weights-improvement-{epoch:02d}-{loss:.4f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "12972/12972 [==============================] - 156s 12ms/step - loss: 2.9664\n",
      "\n",
      "Epoch 00001: loss improved from inf to 2.96642, saving model to weights-improvement-01-2.9664.hdf5\n",
      "Epoch 2/20\n",
      "12972/12972 [==============================] - 175s 13ms/step - loss: 2.8943\n",
      "\n",
      "Epoch 00002: loss improved from 2.96642 to 2.89426, saving model to weights-improvement-02-2.8943.hdf5\n",
      "Epoch 3/20\n",
      "12972/12972 [==============================] - 186s 14ms/step - loss: 2.7029\n",
      "\n",
      "Epoch 00003: loss improved from 2.89426 to 2.70290, saving model to weights-improvement-03-2.7029.hdf5\n",
      "Epoch 4/20\n",
      "12972/12972 [==============================] - 185s 14ms/step - loss: 2.4771\n",
      "\n",
      "Epoch 00004: loss improved from 2.70290 to 2.47710, saving model to weights-improvement-04-2.4771.hdf5\n",
      "Epoch 5/20\n",
      "12972/12972 [==============================] - 161s 12ms/step - loss: 2.2101\n",
      "\n",
      "Epoch 00005: loss improved from 2.47710 to 2.21011, saving model to weights-improvement-05-2.2101.hdf5\n",
      "Epoch 6/20\n",
      "12972/12972 [==============================] - 164s 13ms/step - loss: 1.9850\n",
      "\n",
      "Epoch 00006: loss improved from 2.21011 to 1.98499, saving model to weights-improvement-06-1.9850.hdf5\n",
      "Epoch 7/20\n",
      "12972/12972 [==============================] - 168s 13ms/step - loss: 1.7643\n",
      "\n",
      "Epoch 00007: loss improved from 1.98499 to 1.76431, saving model to weights-improvement-07-1.7643.hdf5\n",
      "Epoch 8/20\n",
      "12972/12972 [==============================] - 168s 13ms/step - loss: 1.6073\n",
      "\n",
      "Epoch 00008: loss improved from 1.76431 to 1.60729, saving model to weights-improvement-08-1.6073.hdf5\n",
      "Epoch 9/20\n",
      "12972/12972 [==============================] - 176s 14ms/step - loss: 1.4774\n",
      "\n",
      "Epoch 00009: loss improved from 1.60729 to 1.47740, saving model to weights-improvement-09-1.4774.hdf5\n",
      "Epoch 10/20\n",
      "12972/12972 [==============================] - 168s 13ms/step - loss: 1.3719\n",
      "\n",
      "Epoch 00010: loss improved from 1.47740 to 1.37189, saving model to weights-improvement-10-1.3719.hdf5\n",
      "Epoch 11/20\n",
      "12972/12972 [==============================] - 160s 12ms/step - loss: 1.2774\n",
      "\n",
      "Epoch 00011: loss improved from 1.37189 to 1.27743, saving model to weights-improvement-11-1.2774.hdf5\n",
      "Epoch 12/20\n",
      "12972/12972 [==============================] - 161s 12ms/step - loss: 1.1821\n",
      "\n",
      "Epoch 00012: loss improved from 1.27743 to 1.18208, saving model to weights-improvement-12-1.1821.hdf5\n",
      "Epoch 13/20\n",
      "12972/12972 [==============================] - 161s 12ms/step - loss: 1.1103\n",
      "\n",
      "Epoch 00013: loss improved from 1.18208 to 1.11034, saving model to weights-improvement-13-1.1103.hdf5\n",
      "Epoch 14/20\n",
      "12972/12972 [==============================] - 161s 12ms/step - loss: 1.0488\n",
      "\n",
      "Epoch 00014: loss improved from 1.11034 to 1.04881, saving model to weights-improvement-14-1.0488.hdf5\n",
      "Epoch 15/20\n",
      "12972/12972 [==============================] - 162s 12ms/step - loss: 0.9953\n",
      "\n",
      "Epoch 00015: loss improved from 1.04881 to 0.99528, saving model to weights-improvement-15-0.9953.hdf5\n",
      "Epoch 16/20\n",
      "12972/12972 [==============================] - 162s 12ms/step - loss: 0.9472\n",
      "\n",
      "Epoch 00016: loss improved from 0.99528 to 0.94719, saving model to weights-improvement-16-0.9472.hdf5\n",
      "Epoch 17/20\n",
      "12972/12972 [==============================] - 161s 12ms/step - loss: 0.9058\n",
      "\n",
      "Epoch 00017: loss improved from 0.94719 to 0.90581, saving model to weights-improvement-17-0.9058.hdf5\n",
      "Epoch 18/20\n",
      "12972/12972 [==============================] - 162s 13ms/step - loss: 0.8644\n",
      "\n",
      "Epoch 00018: loss improved from 0.90581 to 0.86436, saving model to weights-improvement-18-0.8644.hdf5\n",
      "Epoch 19/20\n",
      "12972/12972 [==============================] - 176s 14ms/step - loss: 0.8321\n",
      "\n",
      "Epoch 00019: loss improved from 0.86436 to 0.83205, saving model to weights-improvement-19-0.8321.hdf5\n",
      "Epoch 20/20\n",
      "12972/12972 [==============================] - 174s 13ms/step - loss: 0.8021\n",
      "\n",
      "Epoch 00020: loss improved from 0.83205 to 0.80211, saving model to weights-improvement-20-0.8021.hdf5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x26f1e001b48>"
      ]
     },
     "execution_count": 450,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X, y, epochs=20, batch_size=128, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"weights-improvement-20-0.8021.hdf5\"\n",
    "model.load_weights(filename)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "metadata": {},
   "outputs": [],
   "source": [
    "int_to_char = dict((i, c) for i, c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed:\n",
      "\"  nike coat womens, nike puffer jacket women's, nike leggings girls, grey nike joggers womens, nike j \"\n",
      "oggers wom\n",
      " Done.\n"
     ]
    }
   ],
   "source": [
    "start = numpy.random.randint(0, len(dataX)-1)\n",
    "pattern = dataX[start]\n",
    "print (\"Seed:\")\n",
    "print (\"\\\"\", ''.join([int_to_char[value] for value in pattern]), \"\\\"\")\n",
    "# generate characters\n",
    "for i in range(10):\n",
    "    x = numpy.reshape(pattern, (1, len(pattern), 1))\n",
    "    x = x / float(n_vocab)\n",
    "    prediction = model.predict(x, verbose=0)\n",
    "    index = numpy.argmax(prediction)\n",
    "    result = int_to_char[index]\n",
    "    seq_in = [int_to_char[value] for value in pattern]\n",
    "    sys.stdout.write(result)\n",
    "    pattern.append(index)\n",
    "    pattern = pattern[1:len(pattern)]\n",
    "print (\"\\n Done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Caso 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import LambdaCallback\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras.utils import get_file\n",
    "import random\n",
    "import io\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "metadata": {},
   "outputs": [],
   "source": [
    "Key_Text= ', '.join(Key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_text = re.sub(r'[^\\x00-\\x7f]',r'', Key_Text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpus length: 13072\n",
      "total chars: 36\n"
     ]
    }
   ],
   "source": [
    "print('corpus length:', len(processed_text))\n",
    "\n",
    "chars = sorted(list(set(processed_text)))\n",
    "print('total chars:', len(chars))\n",
    "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
    "indices_char = dict((i, c) for i, c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nb sequences: 4351\n"
     ]
    }
   ],
   "source": [
    "maxlen = 20\n",
    "step = 3\n",
    "sentences = []\n",
    "next_chars = []\n",
    "for i in range(0, len(processed_text) - maxlen, step):\n",
    "    sentences.append(processed_text[i: i + maxlen])\n",
    "    next_chars.append(processed_text[i + maxlen])\n",
    "print('nb sequences:', len(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorization...\n"
     ]
    }
   ],
   "source": [
    "print('Vectorization...')\n",
    "x = np.zeros((len(sentences), maxlen, len(chars)), dtype=np.bool)\n",
    "y = np.zeros((len(sentences), len(chars)), dtype=np.bool)\n",
    "for i, sentence in enumerate(sentences):\n",
    "    for t, char in enumerate(sentence):\n",
    "        x[i, t, char_indices[char]] = 1\n",
    "    y[i, char_indices[next_chars[i]]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n"
     ]
    }
   ],
   "source": [
    "# build the model: a single LSTM\n",
    "print('Build model...')\n",
    "model = Sequential()\n",
    "model.add(LSTM(128, input_shape=(maxlen, len(chars))))\n",
    "model.add(Dense(len(chars), activation='softmax'))\n",
    "\n",
    "optimizer = RMSprop(lr=0.01)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_11 (LSTM)               (None, 128)               84480     \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 36)                4644      \n",
      "=================================================================\n",
      "Total params: 89,124\n",
      "Trainable params: 89,124\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(preds, temperature=1.0):\n",
    "    # helper function to sample an index from a probability array\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "metadata": {},
   "outputs": [],
   "source": [
    "def on_epoch_end(epoch, _):\n",
    "    # Function invoked at end of each epoch. Prints generated text.\n",
    "    print(\"****************************************************************************\")\n",
    "    print('----- Generating text after Epoch: %d' % epoch)\n",
    "\n",
    "    start_index = random.randint(0, 30)\n",
    "    for temperature in [0.2, 0.5, 1.0]:\n",
    "        print('----- temperature:', temperature)\n",
    "\n",
    "        generated = ''\n",
    "        sentence = processed_text[start_index: start_index + maxlen]\n",
    "        generated += sentence\n",
    "        print('----- Generating with seed: \"' + sentence + '\"')\n",
    "        sys.stdout.write(generated)\n",
    "\n",
    "        for i in range(40):\n",
    "            x_pred = np.zeros((1, maxlen, len(chars)))\n",
    "            for t, char in enumerate(sentence):\n",
    "                x_pred[0, t, char_indices[char]] = 1.\n",
    "\n",
    "            preds = model.predict(x_pred, verbose=0)[0]\n",
    "            next_index = sample(preds, temperature)\n",
    "            next_char = indices_char[next_index]\n",
    "\n",
    "            generated += next_char\n",
    "            sentence = sentence[1:] + next_char\n",
    "\n",
    "            sys.stdout.write(next_char)\n",
    "            sys.stdout.flush()\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "4351/4351 [==============================] - 2s 486us/step - loss: 2.8746\n",
      "****************************************************************************\n",
      "----- Generating text after Epoch: 0\n",
      "----- temperature: 0.2\n",
      "----- Generating with seed: \"ts men's, adidas men\"\n",
      "ts men's, adidas mene anike e s ee ane s s nike anike s ade \n",
      "----- temperature: 0.5\n",
      "----- Generating with seed: \"ts men's, adidas men\"\n",
      "ts men's, adidas men, je  are mentke se ele a ide oanee rwr \n",
      "----- temperature: 1.0\n",
      "----- Generating with seed: \"ts men's, adidas men\"\n",
      "ts men's, adidas mentdwominke sr  me anp file s,re tok  rnte\n",
      "Epoch 2/10\n",
      "4351/4351 [==============================] - 2s 439us/step - loss: 1.8527\n",
      "****************************************************************************\n",
      "----- Generating text after Epoch: 1\n",
      "----- temperature: 0.2\n",
      "----- Generating with seed: \"pants men's, adidas \"\n",
      "pants men's, adidas s, nike shorts somen, nike s, nike short\n",
      "----- temperature: 0.5\n",
      "----- Generating with seed: \"pants men's, adidas \"\n",
      "pants men's, adidas surts pants shorts pants swoats, nider, \n",
      "----- temperature: 1.0\n",
      "----- Generating with seed: \"pants men's, adidas \"\n",
      "pants men's, adidas s wort cdiglsnts wpants parts, liddt lir\n",
      "Epoch 3/10\n",
      "4351/4351 [==============================] - 2s 437us/step - loss: 1.3541\n",
      "****************************************************************************\n",
      "----- Generating text after Epoch: 2\n",
      "----- temperature: 0.2\n",
      "----- Generating with seed: \"s, adidas men's pant\"\n",
      "s, adidas men's pants, nike sweats, nike sweat pants, nike s\n",
      "----- temperature: 0.5\n",
      "----- Generating with seed: \"s, adidas men's pant\"\n",
      "s, adidas men's pants trer adidas sweats, nike brer trrcl ni\n",
      "----- temperature: 1.0\n",
      "----- Generating with seed: \"s, adidas men's pant\"\n",
      "s, adidas men's pants wogdnr shorct womens, nike nike panke \n",
      "Epoch 4/10\n",
      "4351/4351 [==============================] - 2s 447us/step - loss: 1.1649\n",
      "****************************************************************************\n",
      "----- Generating text after Epoch: 3\n",
      "----- temperature: 0.2\n",
      "----- Generating with seed: \" pants men's, adidas\"\n",
      " pants men's, adidas suit, nike sweatpants, nike sweatpants,\n",
      "----- temperature: 0.5\n",
      "----- Generating with seed: \" pants men's, adidas\"\n",
      " pants men's, adidas sweatpants, nike swoatpants, nike shoot\n",
      "----- temperature: 1.0\n",
      "----- Generating with seed: \" pants men's, adidas\"\n",
      " pants men's, adidas sweats men, trly nike bogkers, nike fit\n",
      "Epoch 5/10\n",
      "4351/4351 [==============================] - 2s 440us/step - loss: 0.9786\n",
      "****************************************************************************\n",
      "----- Generating text after Epoch: 4\n",
      "----- temperature: 0.2\n",
      "----- Generating with seed: \"n's pants, men's adi\"\n",
      "n's pants, men's adidas sweats, black pants, adidas sweats, \n",
      "----- temperature: 0.5\n",
      "----- Generating with seed: \"n's pants, men's adi\"\n",
      "n's pants, men's adidas homens men, under armour women, nike\n",
      "----- temperature: 1.0\n",
      "----- Generating with seed: \"n's pants, men's adi\"\n",
      "n's pants, men's adidas ssate 'bec, tdints, under armour lin\n",
      "Epoch 6/10\n",
      "4351/4351 [==============================] - 2s 465us/step - loss: 0.8641\n",
      "****************************************************************************\n",
      "----- Generating text after Epoch: 5\n",
      "----- temperature: 0.2\n",
      "----- Generating with seed: \"en's, adidas men's p\"\n",
      "en's, adidas men's pants, adidas suit, adidas suite sweatsui\n",
      "----- temperature: 0.5\n",
      "----- Generating with seed: \"en's, adidas men's p\"\n",
      "en's, adidas men's pants, adidas track pants, nike sweatsuit\n",
      "----- temperature: 1.0\n",
      "----- Generating with seed: \"en's, adidas men's p\"\n",
      "en's, adidas men's pants, adidas track pants, adidas jomen, \n",
      "Epoch 7/10\n",
      "4351/4351 [==============================] - 2s 454us/step - loss: 0.7654\n",
      "****************************************************************************\n",
      "----- Generating text after Epoch: 6\n",
      "----- temperature: 0.2\n",
      "----- Generating with seed: \"en's pants, men's ad\"\n",
      "en's pants, men's adidas, adidas sweatpants, nike sweatpants\n",
      "----- temperature: 0.5\n",
      "----- Generating with seed: \"en's pants, men's ad\"\n",
      "en's pants, men's adidas, adidas sweatpants, adidas nike swe\n",
      "----- temperature: 1.0\n",
      "----- Generating with seed: \"en's pants, men's ad\"\n",
      "en's pants, men's adidas, black neke shoess sccea, under aem\n",
      "Epoch 8/10\n",
      "4351/4351 [==============================] - 2s 469us/step - loss: 0.6911\n",
      "****************************************************************************\n",
      "----- Generating text after Epoch: 7\n",
      "----- temperature: 0.2\n",
      "----- Generating with seed: \"das pants men's, adi\"\n",
      "das pants men's, adidas sweatpants women, nike sweatpants me\n",
      "----- temperature: 0.5\n",
      "----- Generating with seed: \"das pants men's, adi\"\n",
      "das pants men's, adidas suite sweatpants, adidas sweatpants \n",
      "----- temperature: 1.0\n",
      "----- Generating with seed: \"das pants men's, adi\"\n",
      "das pants men's, adidas treck adidas, nike track pants, unde\n",
      "Epoch 9/10\n",
      "4351/4351 [==============================] - 2s 470us/step - loss: 0.6138\n",
      "****************************************************************************\n",
      "----- Generating text after Epoch: 8\n",
      "----- temperature: 0.2\n",
      "----- Generating with seed: \"nts men's, adidas me\"\n",
      "nts men's, adidas mens track pants, adidas sweater alidad, n\n",
      "----- temperature: 0.5\n",
      "----- Generating with seed: \"nts men's, adidas me\"\n",
      "nts men's, adidas mens sweatpants, under armour pants, adida\n",
      "----- temperature: 1.0\n",
      "----- Generating with seed: \"nts men's, adidas me\"\n",
      "nts men's, adidas mens fite coot, nike sweatpants, nike trac\n",
      "Epoch 10/10\n",
      "4351/4351 [==============================] - 2s 471us/step - loss: 0.5613\n",
      "****************************************************************************\n",
      "----- Generating text after Epoch: 9\n",
      "----- temperature: 0.2\n",
      "----- Generating with seed: \" men's pants, men's \"\n",
      " men's pants, men's tracksuit, nike sweatpants, nike sweatpa\n",
      "----- temperature: 0.5\n",
      "----- Generating with seed: \" men's pants, men's \"\n",
      " men's pants, men's tracksuit, nike sweatpants, nike sweatpa\n",
      "----- temperature: 1.0\n",
      "----- Generating with seed: \" men's pants, men's \"\n",
      " men's pants, men's sweat pants, goey nike swiatbout, gher j\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x26f06592448>"
      ]
     },
     "execution_count": 490,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# See https://github.com/tensorflow/tensorflow/issues/31308\n",
    "import logging, os\n",
    "logging.disable(logging.WARNING)\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
    "\n",
    "# Fit the model\n",
    "print_callback = LambdaCallback(on_epoch_end=on_epoch_end)\n",
    "\n",
    "model.fit(x, y,\n",
    "          batch_size=128,\n",
    "          epochs=10,\n",
    "          callbacks=[print_callback])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
